/home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py:57: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:54.)
  torch.clone(self.weight.detach().type(dtype).to_sparse_csr())
[2023-09-12 20:01:32,882] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:32,903] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:32,905] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
Benchmarking batch size 1024 with sparsity 90 and num_threads 1
[2023-09-12 20:01:34,965] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0
[2023-09-12 20:01:35,235] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/4e/c4eur5x2znt44g6d3im7ywdrncyzoka5wach7hljrenap4a3hpgw.py
[2023-09-12 20:01:35,235] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0
[2023-09-12 20:01:35,236] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:35,242] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_0 <eval_with_key>.5 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:01:35,243] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:01:35,243] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_0)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:35,245] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520638383200)'],
                'obj_weakref': <weakref at 0x7fcd82c88fe0; to 'CondensedLinearStructured' at 0x7fcd82bc0460>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce348cb6f0; to 'Tensor' at 0x7fcd9738dee0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:37,691] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:37,712] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:37,714] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:37,748] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 1
[2023-09-12 20:01:37,781] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:01:37,874] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/ad/cadh72ahqsxxnjenmvr4uqmp7dwf3f65jyx6szfpgty4hlfxghqp.py
[2023-09-12 20:01:37,875] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 1
[2023-09-12 20:01:37,875] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:37,877] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_1 <eval_with_key>.11 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:01:37,877] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:01:37,878] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_1)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:37,878] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520638384448)'],
                'obj_weakref': <weakref at 0x7fce3484db70; to 'CondensedLinearFineGrained' at 0x7fcd82bc0940>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce348cb6f0; to 'Tensor' at 0x7fcd9738dee0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:42,990] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:43,006] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:43,008] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:43,051] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 2
[2023-09-12 20:01:43,239] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/hu/chufvgtddgsgp3xjhnvbb4unemdrabotehom6czrjln2nddsowk4.py
[2023-09-12 20:01:43,240] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 2
[2023-09-12 20:01:43,240] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:43,241] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_2 <eval_with_key>.17 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:01:43,242] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:01:43,242] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_2)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:43,243] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520638387712)'],
                'obj_weakref': <weakref at 0x7fce348463e0; to 'CondensedLinearStructured' at 0x7fcd82bc1600>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce34844cc0; to 'Tensor' at 0x7fcd82c8a0c0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:45,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:45,618] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:45,620] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:45,653] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 3
[2023-09-12 20:01:45,673] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:01:45,739] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/m5/cm52ovgaroipdrh6m6qwjyxp4zsqaozjv2f7f6z5abwrtj6jlm4v.py
[2023-09-12 20:01:45,739] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 3
[2023-09-12 20:01:45,739] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:45,741] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_3 <eval_with_key>.23 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:01:45,742] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:01:45,742] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_3)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:45,743] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520639938272)'],
                'obj_weakref': <weakref at 0x7fcd82cae2f0; to 'CondensedLinearFineGrained' at 0x7fcd82d3bee0>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce34844cc0; to 'Tensor' at 0x7fcd82c8a0c0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:50,563] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:50,578] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:50,580] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:50,622] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 4
[2023-09-12 20:01:50,808] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/s7/cs7ug2mfw2kqh2v67qjadgmql7ozi4vt5gaad6nwke35t375qn5p.py
[2023-09-12 20:01:50,809] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 4
[2023-09-12 20:01:50,809] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:50,810] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_4 <eval_with_key>.29 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:01:50,810] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:01:50,811] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_4)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:50,812] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520639925840)'],
                'obj_weakref': <weakref at 0x7fcd82c21120; to 'CondensedLinearStructured' at 0x7fcd82d38e50>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fcd82c22070; to 'Tensor' at 0x7fce347f8360>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:53,152] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:53,168] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:53,170] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:53,203] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 5
[2023-09-12 20:01:53,222] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:01:53,316] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/wt/cwtmmgrmq5mjr37dqjma62jsjmwvszno4yrmbib6gxxtzjcn5vkr.py
[2023-09-12 20:01:53,316] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 5
[2023-09-12 20:01:53,316] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:53,318] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_5 <eval_with_key>.35 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:01:53,319] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:01:53,319] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_5)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:53,320] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520640279792)'],
                'obj_weakref': <weakref at 0x7fcd82d8b060; to 'CondensedLinearFineGrained' at 0x7fcd82d8f4f0>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fcd82c22070; to 'Tensor' at 0x7fce347f8360>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:57,294] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:57,313] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:57,315] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:57,362] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 6
[2023-09-12 20:01:57,553] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/ev/cev7a6i4fjm4kdtdkiwuxuep7jw7qxtmpmpaiznjssv43lehfbvw.py
[2023-09-12 20:01:57,554] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 6
[2023-09-12 20:01:57,554] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:01:57,555] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_6 <eval_with_key>.41 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:01:57,556] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:01:57,556] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_6)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:01:57,557] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523624705968)'],
                'obj_weakref': <weakref at 0x7fce33792660; to 'CondensedLinearStructured' at 0x7fce34bbafb0>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce33791b20; to 'Tensor' at 0x7fcd82d8b2e0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:01:59,919] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:01:59,934] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:01:59,936] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:01:59,968] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 7
[2023-09-12 20:01:59,988] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:00,081] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/gw/cgwukit2v6ooylzs42qae3umd2eq46zihvc7xcrvdni7xjf7bvat.py
[2023-09-12 20:02:00,082] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 7
[2023-09-12 20:02:00,082] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:00,084] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_7 <eval_with_key>.47 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:00,084] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:00,085] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_7)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:00,085] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520640212576)'],
                'obj_weakref': <weakref at 0x7fce34a99260; to 'CondensedLinearFineGrained' at 0x7fcd82d7ee60>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce33791b20; to 'Tensor' at 0x7fcd82d8b2e0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:03,587] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:03,602] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:03,604] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:03,646] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 8
[2023-09-12 20:02:03,819] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/i2/ci2p353o66rl3ddyrg7gtjctyf7qxh46bw2gah5yruqrt3vakm5v.py
[2023-09-12 20:02:03,820] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 8
[2023-09-12 20:02:03,820] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:03,821] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_8 <eval_with_key>.53 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:03,822] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:03,822] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_8)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:03,823] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523622873408)'],
                'obj_weakref': <weakref at 0x7fce33d4a250; to 'CondensedLinearStructured' at 0x7fce349fb940>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce33d4a570; to 'Tensor' at 0x7fce33d29800>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:06,191] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:06,207] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:06,209] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:06,242] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 9
[2023-09-12 20:02:06,262] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:06,356] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/3w/c3wvmgi6mvt4cmucolcholmtaoitprwxiyq626k2f25zyvan7hyr.py
[2023-09-12 20:02:06,356] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 9
[2023-09-12 20:02:06,356] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:06,358] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_9 <eval_with_key>.59 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:06,359] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:06,359] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_9)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:06,360] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523621290944)'],
                'obj_weakref': <weakref at 0x7fce337adb70; to 'CondensedLinearFineGrained' at 0x7fce348793c0>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce33d4a570; to 'Tensor' at 0x7fce33d29800>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:11,132] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:11,148] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:11,150] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:11,193] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 10
[2023-09-12 20:02:11,376] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/xx/cxxdskh5patb3jaokd6gmb6mlajbxxnoisrnqpxe53vwdhmveeef.py
[2023-09-12 20:02:11,376] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 10
[2023-09-12 20:02:11,377] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:11,378] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_10 <eval_with_key>.65 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:11,379] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:11,379] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_10)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:11,380] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523623430032)'],
                'obj_weakref': <weakref at 0x7fce3382e2f0; to 'CondensedLinearStructured' at 0x7fce34a83790>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce3382db70; to 'Tensor' at 0x7fce337a1b70>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:14,040] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:14,056] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:14,058] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:14,093] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 11
[2023-09-12 20:02:14,112] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:14,199] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/7d/c7dxxapc5r2bcoxyx7pegvi53yj5my3x72ck4pcl3pchetiz7gl3.py
[2023-09-12 20:02:14,199] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 11
[2023-09-12 20:02:14,199] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:14,201] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_11 <eval_with_key>.71 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:14,202] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:14,202] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_11)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:14,203] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523623428064)'],
                'obj_weakref': <weakref at 0x7fce33d6ffb0; to 'CondensedLinearFineGrained' at 0x7fce34a82fe0>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce3382db70; to 'Tensor' at 0x7fce337a1b70>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:18,573] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:18,589] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:18,590] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:18,633] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 12
[2023-09-12 20:02:18,823] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/2c/c2ccfxrqfbuazovx5ww5dr7bo2cjsay2qyni2irhpwlboknphvq4.py
[2023-09-12 20:02:18,823] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 12
[2023-09-12 20:02:18,823] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:18,824] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_12 <eval_with_key>.77 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:18,825] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:18,825] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_12)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:18,826] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520640207488)'],
                'obj_weakref': <weakref at 0x7fce337a3ab0; to 'CondensedLinearStructured' at 0x7fcd82d7da80>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce337a2160; to 'Tensor' at 0x7fce33810900>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:21,372] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:21,389] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:21,391] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:21,426] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 13
[2023-09-12 20:02:21,446] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:21,539] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/w3/cw3m2aydsfvrz2s2ms6ocyklpsdyfzvejkwoqisycqz7uwibhvz7.py
[2023-09-12 20:02:21,540] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 13
[2023-09-12 20:02:21,540] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:21,542] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_13 <eval_with_key>.83 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:21,543] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:21,543] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_13)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:21,544] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523621158816)'],
                'obj_weakref': <weakref at 0x7fce337a1210; to 'CondensedLinearFineGrained' at 0x7fce34858fa0>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce337a2160; to 'Tensor' at 0x7fce33810900>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:25,925] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:25,941] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:25,943] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:25,986] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 14
[2023-09-12 20:02:26,159] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/he/chegmcagnnoolvwlgn45l2iwcpxbyhac7cng6ggymeajzc3s56px.py
[2023-09-12 20:02:26,159] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 14
[2023-09-12 20:02:26,160] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:26,161] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_14 <eval_with_key>.89 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:26,161] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:26,161] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_14)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:26,162] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140520638469632)'],
                'obj_weakref': <weakref at 0x7fce33804860; to 'CondensedLinearStructured' at 0x7fcd82bd5600>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce338046d0; to 'Tensor' at 0x7fce337a2110>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:28,700] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:28,716] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:28,718] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:28,753] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 15
[2023-09-12 20:02:28,773] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:28,858] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/ky/ckyq6yqbbde6t4egyyokae2xjkvzpaelvo3mnpog3fzu73p7jnd4.py
[2023-09-12 20:02:28,858] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 15
[2023-09-12 20:02:28,859] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:28,860] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_15 <eval_with_key>.95 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:28,861] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:28,861] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_15)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:28,862] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523604072560)'],
                'obj_weakref': <weakref at 0x7fce33810090; to 'CondensedLinearFineGrained' at 0x7fce3380d870>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce338046d0; to 'Tensor' at 0x7fce337a2110>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:33,048] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:33,063] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:33,065] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:33,106] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 16
[2023-09-12 20:02:33,296] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/pm/cpm7tubwtujcegtbhhfzfjhsh5szslsg3ucepwor7nzhnoe35rrl.py
[2023-09-12 20:02:33,296] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 16
[2023-09-12 20:02:33,297] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:33,298] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_16 <eval_with_key>.101 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:33,298] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:33,298] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_16)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:33,299] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523621165968)'],
                'obj_weakref': <weakref at 0x7fce335260c0; to 'CondensedLinearStructured' at 0x7fce3485ab90>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce335262a0; to 'Tensor' at 0x7fce337ac9f0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:35,761] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:35,777] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:35,779] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:35,812] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 17
[2023-09-12 20:02:35,832] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:35,926] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/ec/cecp56m2ijkn3e6pzex77unmtuyx56fdqxcrk7m7pheddcuj22d3.py
[2023-09-12 20:02:35,926] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 17
[2023-09-12 20:02:35,926] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:35,928] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_17 <eval_with_key>.107 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:35,929] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:35,929] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_17)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:35,930] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523609277056)'],
                'obj_weakref': <weakref at 0x7fce3382fc90; to 'CondensedLinearFineGrained' at 0x7fce33d04280>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce335262a0; to 'Tensor' at 0x7fce337ac9f0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:40,727] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:40,745] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:40,747] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:40,796] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 18
[2023-09-12 20:02:40,891] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/hc/chce2xke6ama3r2mjevv5bh4z57ze2ijpvu5c6gto5yw4gm2whsu.py
[2023-09-12 20:02:40,892] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 18
[2023-09-12 20:02:40,892] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:40,893] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_18 <eval_with_key>.113 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:40,894] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:40,894] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_18)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:40,895] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523609787216)'],
                'obj_weakref': <weakref at 0x7fce3352b510; to 'CondensedLinearStructured' at 0x7fce33d80b50>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce3352b970; to 'Tensor' at 0x7fce33502ca0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:43,379] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:43,395] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:43,397] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:43,431] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 19
[2023-09-12 20:02:43,451] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:43,547] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/hh/chhw2jdgv26preumpnefpywtwedudbm6wilyojgonzo63u2n4ijs.py
[2023-09-12 20:02:43,547] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 19
[2023-09-12 20:02:43,548] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:43,549] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_19 <eval_with_key>.119 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:43,550] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:43,550] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_19)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:43,551] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523609791296)'],
                'obj_weakref': <weakref at 0x7fce33525030; to 'CondensedLinearFineGrained' at 0x7fce33d81b40>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce3352b970; to 'Tensor' at 0x7fce33502ca0>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:47,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:47,703] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:47,705] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:47,746] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 20
[2023-09-12 20:02:47,837] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/s4/cs4iy3hrhwjxnkz6qqjtzacuyfbcpfr52w2ml2jvlk54otkdcbzo.py
[2023-09-12 20:02:47,837] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 20
[2023-09-12 20:02:47,837] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:47,838] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_20 <eval_with_key>.125 opcode         name         target                      args                               kwargs
-------------  -----------  --------------------------  ---------------------------------  --------
placeholder    input_1      input                       ()                                 {}
get_attr       self_weight  self_weight                 ()                                 {}
get_attr       self_bias    self_bias                   ()                                 {}
call_function  linear       <built-in function linear>  (input_1, self_weight, self_bias)  {}
output         output       output                      ((linear,),)                       {}

[2023-09-12 20:02:47,839] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 82           0 LOAD_GLOBAL              0 (F)
              2 LOAD_METHOD              1 (linear)
              4 LOAD_FAST                1 (input)
              6 LOAD_FAST                0 (self)
              8 LOAD_ATTR                2 (weight)
             10 LOAD_FAST                0 (self)
             12 LOAD_ATTR                3 (bias)
             14 CALL_METHOD              3
             16 RETURN_VALUE

 
[2023-09-12 20:02:47,839] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 81 
 81           0 LOAD_GLOBAL              4 (__compiled_fn_20)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:47,840] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523604079184)'],
                'obj_weakref': <weakref at 0x7fce334ea070; to 'CondensedLinearStructured' at 0x7fce3380f250>
                'guarded_class': <weakref at 0x7fcd82fc4db0; to 'type' at 0x7d16dc0 (CondensedLinearStructured)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce33529530; to 'Tensor' at 0x7fce33841620>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'F' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
[2023-09-12 20:02:50,182] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward
[2023-09-12 20:02:50,198] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2023-09-12 20:02:50,200] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper
[2023-09-12 20:02:50,233] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 21
[2023-09-12 20:02:50,252] torch._inductor.codegen.triton: [INFO] schedule: [SchedulerNode(name='buf0'), <class 'torch._inductor.codegen.triton.DisableReduction'>, SchedulerNode(name='buf1'), <class 'torch._inductor.codegen.triton.EnableReduction'>]
[2023-09-12 20:02:50,342] torch._inductor.graph: [INFO] Output code: /tmp/torchinductor_user/q6/cq6thgqy2wnlfg63uj2jhbgtuilzuydchgzastxe6u2e7r5nnhw2.py
[2023-09-12 20:02:50,343] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 21
[2023-09-12 20:02:50,343] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper
[2023-09-12 20:02:50,345] torch._dynamo.output_graph: [INFO] TRACED GRAPH
 __compiled_fn_21 <eval_with_key>.131 opcode         name                   target                                                  args                                                   kwargs
-------------  ---------------------  ------------------------------------------------------  -----------------------------------------------------  -----------
placeholder    input_1                input                                                   ()                                                     {}
get_attr       self_condensed_weight  self_condensed_weight                                   ()                                                     {}
get_attr       self_input_mask        self_input_mask                                         ()                                                     {}
call_function  getitem                <built-in function getitem>                             (input_1, (slice(None, None, None), self_input_mask))  {}
call_function  mul                    <built-in function mul>                                 (self_condensed_weight, getitem)                       {}
call_function  sum_1                  <built-in method sum of type object at 0x7fcea3243880>  (mul,)                                                 {'axis': 2}
get_attr       self_bias              self_bias                                               ()                                                     {}
call_function  add                    <built-in function add>                                 (sum_1, self_bias)                                     {}
output         output                 output                                                  ((add,),)                                              {}

[2023-09-12 20:02:50,346] torch._dynamo.convert_frame: [INFO] ORIGINAL BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
117           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_ATTR                1 (sum)
              4 LOAD_FAST                0 (self)
              6 LOAD_ATTR                2 (condensed_weight)
              8 LOAD_FAST                1 (input)
             10 LOAD_CONST               0 (None)
             12 LOAD_CONST               0 (None)
             14 BUILD_SLICE              2
             16 LOAD_FAST                0 (self)
             18 LOAD_ATTR                3 (input_mask)
             20 BUILD_TUPLE              2
             22 BINARY_SUBSCR
             24 BINARY_MULTIPLY
             26 LOAD_CONST               1 (2)
             28 LOAD_CONST               2 (('axis',))
             30 CALL_FUNCTION_KW         2

118          32 LOAD_FAST                0 (self)
             34 LOAD_ATTR                4 (bias)

117          36 BINARY_ADD

116          38 RETURN_VALUE

 
[2023-09-12 20:02:50,346] torch._dynamo.convert_frame: [INFO] MODIFIED BYTECODE forward /home/user/condensed-sparsity/src/condensed_sparsity/v2/condensed_linear.py line 115 
115           0 LOAD_GLOBAL              5 (__compiled_fn_21)
              2 LOAD_FAST                1 (input)
              4 CALL_FUNCTION            1
              6 UNPACK_SEQUENCE          1
              8 RETURN_VALUE

 
[2023-09-12 20:02:50,347] torch._dynamo.convert_frame: [INFO] GUARDS:
 - 
            local 'self' NN_MODULE
            {
                'guard_types': ['ID_MATCH'],
                'code': ['___check_obj_id(self, 140523604068288)'],
                'obj_weakref': <weakref at 0x7fcde1ad5fd0; to 'CondensedLinearFineGrained' at 0x7fce3380c7c0>
                'guarded_class': <weakref at 0x7fcd82fc5e90; to 'type' at 0x7d1f600 (CondensedLinearFineGrained)>
            }
            
 - 
            local 'input' TENSOR_MATCH
            {
                'guard_types': ['TENSOR_MATCH'],
                'code': None,
                'obj_weakref': <weakref at 0x7fce33529530; to 'Tensor' at 0x7fce33841620>
                'guarded_class': <weakref at 0x7fcd9673c9f0; to 'torch._C._TensorMeta' at 0x553ca40 (Tensor)>
            }
            
 - 
            global 'torch' FUNCTION_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.bias' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.input_mask' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
 - 
            local_nn_module 'self.condensed_weight' TENSOR_MATCH
            {
                'guard_types': None,
                'code': None,
                'obj_weakref': None
                'guarded_class': None
            }
            
Benchmarking batch size 512 with sparsity 90 and num_threads 1
Benchmarking batch size 256 with sparsity 90 and num_threads 1
Benchmarking batch size 128 with sparsity 90 and num_threads 1
Benchmarking batch size 64 with sparsity 90 and num_threads 1
Benchmarking batch size 32 with sparsity 90 and num_threads 1
Benchmarking batch size 16 with sparsity 90 and num_threads 1
Benchmarking batch size 8 with sparsity 90 and num_threads 1
Benchmarking batch size 4 with sparsity 90 and num_threads 1
Benchmarking batch size 2 with sparsity 90 and num_threads 1
Benchmarking batch size 1 with sparsity 90 and num_threads 1
[---------------------------------------- Condensed Linear @ 90 with 1 threads ----------------------------------------]
                     |  Structured sparsity @ 90  |  Fine-grained + structured sparsity @ 90  |  Dense benchmark - Eager
1 threads: -------------------------------------------------------------------------------------------------------------
      1024   x 768   |  [92m[1m         103.2          [0m[0m  |  [31m[1m                 1765.4                [0m[0m  |  [2m[91m         114.3         [0m[0m
      512    x 768   |  [92m[1m         103.8          [0m[0m  |  [31m[1m                  896.2                [0m[0m  |            60.7         
      256    x 768   |  [34m[1m         106.3          [0m[0m  |  [2m[91m                  463.5                [0m[0m  |            62.2         
      128    x 768   |  [34m[1m         104.2          [0m[0m  |  [2m[91m                  237.3                [0m[0m  |            60.9         
      64     x 768   |  [92m[1m         103.2          [0m[0m  |                    135.6                  |  [34m[1m          56.1         [0m[0m
      32     x 768   |           115.7            |  [34m[1m                  115.2                [0m[0m  |  [34m[1m          58.1         [0m[0m
      16     x 768   |           114.5            |  [34m[1m                  112.5                [0m[0m  |  [34m[1m          58.6         [0m[0m
      8      x 768   |           114.8            |  [92m[1m                  105.8                [0m[0m  |  [34m[1m          58.7         [0m[0m
      4      x 768   |  [34m[1m         109.7          [0m[0m  |                    131.0                  |            66.0         
      2      x 768   |  [34m[1m         108.4          [0m[0m  |  [92m[1m                  105.7                [0m[0m  |  [92m[1m          54.3         [0m[0m
      1      x 768   |  [92m[1m         102.8          [0m[0m  |  [34m[1m                  108.4                [0m[0m  |  [34m[1m          55.7         [0m[0m

Times are in microseconds (us).
