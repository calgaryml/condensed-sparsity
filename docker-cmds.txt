# Hacks until we have compose installed:
# docker build --file ./Dockerfile -t rigl-agent --shm-size=16gb .
# docker create --cpus 12 --memory 30G --env-file .env --gpus '"device=1"' -i -t rigl-sweep-agent:latest
# docker create --env-file .env --gpus '"device=0"' -i -t rigl-sweep-agent:latest
# docker run --cpus 12 --memory 64G --env-file .env --gpus '"device=0"' -i -t --name gpu-0 rigl-agent:latest
# docker run --cpus 12 --memory 30G --env-file .env --gpus '"device=1"' -i -t -d --name gpu-1 rigl-sweep-agent:latest
# docker run --cpus 12 --memory 30G --env-file .env --gpus '"device=2"' -i -t --name gpu-2 rigl-sweep-agent:latest
# docker run --cpus 12 --memory 30G --env-file .env --gpus '"device=3"' -i -t --name gpu-3 rigl-sweep-agent:latest
# docker run -itd --env-file ./.env --mount source=/home/mike/condensed-sparsity,target=/home/condensed-sparsity,type=bind --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind --gpus all --shm-size 16G --name cond-rigl-dev ac605476a22f

gsutil -m cp -R gs://condensed_sparsity/artifacts/checkpoints ./

louise
100.90.128.40
172.20.206.69

torchrun \
   --nnodes=2 \
   --nproc-per-node=2 \
   --max-restarts=1 \
   --rdzv-id=100 \
   --rdzv-backend=c10d \
   --rdzv-endpoint=louise:29400 \
   train_rigl.py training.dry_run=True

torchrun \
   --nnodes=2 \
   --nproc-per-node=2 \
   --max-restarts=1 \
   --rdzv-id=100 \
   --rdzv-backend=c10d \
   --rdzv-endpoint=172.20.206.69:29400 \
   train_rigl.py experiment.resume_from_checkpoint=True experiment.run_id=x3nse9y3


#bow/moraine
#bow
NCCL_IB_DISABLE=1 NCCL_P2P_DISABLE=1 NCCL_DEBUG=INFO TORCH_CPP_LOG_LEVEL=INFO TORCH_DISTRIBUTED_DEBUG=INFO NCCL_SOCKET_IFNAME=enp130s0f0 \
torchrun \
  --nnodes=2 \
  --nproc-per-node=2 \
  --max-restarts=0 \
  --rdzv-id=100 \
  --rdzv-backend=c10d \
  --rdzv-endpoint=localhost:29400 \
  train_rigl.py experiment.resume_from_checkpoint=True experiment.run_id=2ccexxga

# moraine
NCCL_SOCKET_IFNAME=enp134s0f1np1 torchrun \
  --nnodes=2 \
  --nproc-per-node=2 \
  --max-restarts=0 \
  --rdzv-id=100 \
  --rdzv-backend=c10d \
  --rdzv-endpoint=louise.local.calgaryml.com:29400 \
  train_rigl.py experiment.resume_from_checkpoint=True experiment.run_id=yt8arphp
  

# Louise
NCCL_DEBUG=INFO TORCH_CPP_LOG_LEVEL=INFO TORCH_DISTRIBUTED_DEBUG=INFO NCCL_SOCKET_IFNAME=enp129s0f0 \
torchrun \
  --nnodes=2 \
  --nproc-per-node=2 \
  --max-restarts=0 \
  --rdzv-id=100 \
  --rdzv-backend=c10d \
  --rdzv-endpoint=localhost:29400 \
  train_rigl.py experiment.resume_from_checkpoint=True experiment.run_id=yt8arphp


torchrun \
  --nnodes=1 \
  --nproc-per-node=4 \
  --max-restarts=0 \
  --rdzv-id=100 \
  --rdzv-backend=c10d \
  --rdzv-endpoint=localhost:29400 \
  train_rigl.py experiment.resume_from_checkpoint=True experiment.run_id=tkndtcux


docker build --file ./Dockerfile.gcs -t mklasby/condensed-sparsity:rigl-gcs --shm-size=16gb .
docker push mklasby/condensed-sparsity:rigl-gcs
docker run -itd --name gcp-test --mount 'type=bind,src=/home/mike/.config/gcloud,dst=/root/.config/gcloud' gcs:latest

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=0"' \
  --shm-size 16G \
  --name gpu-0 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/jporfu3v

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=1"' \
  --shm-size 16G \
  --name gpu-1 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/jporfu3v

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=2"' \
  --shm-size 16G \
  --name gpu-2 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/jporfu3v

docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=3"' \
  --shm-size 16G \
  --name gpu-3 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  wandb agent condensed-sparsity/condensed-rigl/jporfu3v

  
python ./train_rigl.py \
  rigl.dense_allocation=0.01 \
  rigl.delta=1600 \
  training.batch_size=256 \
  training.max_steps=512000 \
  training.lr=0.1 \
  compute.world_size=2 \
  rigl.grad_accumulation_n=16 \
  rigl.min_salient_weights_per_neuron=10

python ./train_rigl.py \
  dataset=cifar10 \
  model=resnet18 \
  rigl.dense_allocation=0.01 \
  rigl.delta=100 \
  rigl.grad_accumulation_n=1 \
  rigl.min_salient_weights_per_neuron=0.05 \
  training.batch_size=128 \
  training.max_steps=null \
  training.weight_decay=5.0e-4 \
  training.label_smoothing=0 \
  training.lr=0.1 \
  training.epochs=250 \
  training.warm_up_steps=0 \
  training.scheduler=step_lr \
  training.step_size=77 \
  training.gamma=0.2 \
  compute.distributed=False



docker run -itd --env-file ./.env \
  --mount source=/home/mike/condensed-sparsity/artifacts,target=/home/user/condensed-sparsity/artifacts,type=bind \
  --mount source=/home/mike/condensed-sparsity/configs,target=/home/user/condensed-sparsity/configs,type=bind \
  --mount source=/home/mike/condensed-sparsity/wandb,target=/home/user/condensed-sparsity/wandb,type=bind \
  --mount source=/datasets/ILSVRC2012,target=/datasets/ILSVRC2012,type=bind \
  --mount source=/scratch/datasets/ILSVRC2012,target=/scratch/datasets/ILSVRC2012,type=bind \
  --gpus '"device=0"' \
  --shm-size 16G \
  --name gpu-0 \
  --cpus 12 \
  --memory 30G \
  rigl-agent:latest \
  python ./train_rigl.py \
  dataset=cifar10 \
  model=wide_resnet22 \
  rigl.dense_allocation=0.01 \
  rigl.delta=100 \
  rigl.grad_accumulation_n=1 \
  rigl.min_salient_weights_per_neuron=0.1 \
  training.batch_size=128 \
  training.max_steps=null \
  training.weight_decay=5.0e-4 \
  training.label_smoothing=0 \
  training.lr=0.1 \
  training.epochs=250 \
  training.warm_up_steps=0 \
  training.scheduler=step_lr \
  training.step_size=77 \
  training.gamma=0.2 \
  compute.distributed=False \
  rigl.use_sparse_initialization=True \
  rigl.init_method_str=grad_flow_init \
  training.seed=42

# Imagenet
python ./train_rigl.py \
dataset=imagenet \
model=resnet50 \
rigl.dense_allocation=0.01 \
rigl.delta=800 \
rigl.grad_accumulation_n=8 \
rigl.min_salient_weights_per_neuron=0.5 \
training.batch_size=512 \
training.max_steps=256000 \
training.weight_decay=0.0001 \
training.label_smoothing=0.1 \
training.lr=0.2 \
training.epochs=104 \
training.warm_up_steps=5 \
training.scheduler=step_lr_with_warm_up \
training.step_size=[30,70,90] \
training.gamma=0.1 \
compute.distributed=True \
compute.world_size=4 \
rigl.use_sparse_initialization=True \
rigl.init_method_str=grad_flow_init

# Imagenet
python ./train_rigl.py \
experiment.resume_from_checkpoint=True \
experiment.run_id=f63nx0vb


# Imagenet
python ./train_rigl.py \
dataset=imagenet \
model=resnet50 \
rigl.dense_allocation=0.01 \
rigl.delta=800 \
rigl.grad_accumulation_n=8 \
rigl.min_salient_weights_per_neuron=0.3 \
rigl.keep_first_layer_dense=True \
training.batch_size=512 \
training.max_steps=256000 \
training.weight_decay=0.0001 \
training.label_smoothing=0.1 \
training.lr=0.2 \
training.epochs=104 \
training.warm_up_steps=5 \
training.scheduler=step_lr_with_warm_up \
training.step_size=[30,70,90] \
training.gamma=0.1 \
compute.distributed=True \
compute.world_size=4 \
rigl.use_sparse_initialization=True \
rigl.init_method_str=grad_flow_init
