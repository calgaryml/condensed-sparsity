{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/mike/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maskrcnn\n",
      "/home/mike/condensed-sparsity\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import dotenv\n",
    "import omegaconf\n",
    "import hydra\n",
    "import logging\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from datetime import date\n",
    "import dotenv\n",
    "import os\n",
    "import pathlib\n",
    "from typing import Dict, Any\n",
    "from copy import deepcopy\n",
    "\n",
    "from rigl_torch.models import ModelFactory\n",
    "from rigl_torch.rigl_scheduler import RigLScheduler\n",
    "from rigl_torch.rigl_constant_fan import RigLConstFanScheduler\n",
    "from rigl_torch.datasets import get_dataloaders\n",
    "from rigl_torch.optim import (\n",
    "    get_optimizer,\n",
    "    get_lr_scheduler,\n",
    ")\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.utils.rigl_utils import get_T_end, get_fan_in_after_ablation, get_conv_idx_from_flat_idx\n",
    "from rigl_torch.meters import SegmentationMeter\n",
    "from hydra import initialize, compose\n",
    "from rigl_torch.utils.dist_utils import get_steps_to_accumulate_grad\n",
    "from rigl_torch.utils.wandb_utils import init_wandb\n",
    "\n",
    "\n",
    "with initialize(\"../configs\", version_base=\"1.2.0\"):\n",
    "    cfg = compose(\n",
    "        \"config.yaml\",\n",
    "        overrides=[\n",
    "            \"compute.distributed=False\",\n",
    "            \"dataset=coco\",\n",
    "            \"model=maskrcnn\",\n",
    "            \"training.test_batch_size=10\",\n",
    "            \"training.save_model=False\",\n",
    "            \"wandb.log_to_wandb=False\"\n",
    "            ])\n",
    "dotenv.load_dotenv(\"../.env\", override=True)\n",
    "os.environ[\"IMAGE_NET_PATH\"]\n",
    "print(cfg.model.name)\n",
    "print(cfg.paths.base)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_cuda': False, 'cuda_kwargs': {'num_workers': '${ oc.decode:${oc.env:NUM_WORKERS} }', 'pin_memory': True}, 'distributed': False, 'world_size': 4, 'dist_backend': 'nccl'}\n",
      "No logging to WANDB! See cfg.wandb.log_to_wandb\n",
      "loading to device rank: 0\n",
      "/home/mike/condensed-sparsity\n",
      "loading annotations into memory...\n",
      "Done (t=8.54s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.80s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/models/model_factory.py:Loading model maskrcnn/coco using <function get_maskrcnn at 0x7fe708b6cee0> with args: () and kwargs: {'diet': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/models/detection/backbone_utils.py:160: UserWarning: Changing trainable_backbone_layers has not effect if neither pretrained nor pretrained_backbone have been set to True, falling back to trainable_backbone_layers=5 so that all layers are trainable\n",
      "  warnings.warn(\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 62 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 63 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 1 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 3 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 4 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 5 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 7 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 8 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 10 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 73 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 66 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 11 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 13 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 15 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 17 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 18 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 20 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 21 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 23 set to 0.0\n",
      "INFO:/home/mike/condensed-sparsity/src/rigl_torch/rigl_scheduler.py:Sparsity of layer at index 0 set to 0.0\n"
     ]
    }
   ],
   "source": [
    "rank=0\n",
    "checkpoint=None\n",
    "if checkpoint is not None:\n",
    "    run_id = checkpoint.run_id\n",
    "    optimizer_state = checkpoint.optimizer\n",
    "    scheduler_state = checkpoint.scheduler\n",
    "    pruner_state = checkpoint.pruner\n",
    "    model_state = checkpoint.model\n",
    "    cfg = checkpoint.cfg\n",
    "    wandb_init_resume = \"must\"\n",
    "else:\n",
    "    run_id, optimizer_state, scheduler_state, pruner_state, model_state, wandb_init_resume = (\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None,\n",
    "        None\n",
    "    )\n",
    "\n",
    "if \"diet\" not in cfg.rigl:\n",
    "    with omegaconf.open_dict(cfg):\n",
    "        cfg.rigl.diet = None\n",
    "if \"keep_first_layer_dense\" not in cfg.rigl:\n",
    "    with omegaconf.open_dict(cfg):\n",
    "        cfg.rigl.keep_first_layer_dense = False\n",
    "print(cfg.compute)\n",
    "cfg.compute.distributed=False\n",
    "wandb_init_kwargs = dict(resume=wandb_init_resume, id=run_id)\n",
    "run = init_wandb(cfg, wandb_init_kwargs)\n",
    "pl.seed_everything(cfg.training.seed)\n",
    "use_cuda = not cfg.compute.no_cuda and torch.cuda.is_available()\n",
    "if not use_cuda:\n",
    "    raise SystemError(\"GPU has stopped responding...waiting to die!\")\n",
    "    logger.warning(\n",
    "        \"Using CPU! Verify cfg.compute.no_cuda and \"\n",
    "        \"torch.cuda.is_available() are properly set if this is unexpected\"\n",
    "    )\n",
    "\n",
    "if cfg.compute.distributed and use_cuda:\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "else:\n",
    "    print(f\"loading to device rank: {rank}\")\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "if not use_cuda:\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(cfg.paths.base)\n",
    "train_loader, test_loader = get_dataloaders(cfg)\n",
    "\n",
    "model = ModelFactory.load_model(\n",
    "    model=cfg.model.name, dataset=cfg.dataset.name, diet=cfg.rigl.diet\n",
    ")\n",
    "model.to(device)\n",
    "if cfg.compute.distributed:\n",
    "    model = DistributedDataParallel(model, device_ids=[rank])\n",
    "if model_state is not None:\n",
    "    try:\n",
    "        model.load_state_dict(model_state)\n",
    "    except RuntimeError:\n",
    "        model_state = checkpoint.get_single_process_model_state_from_distributed_state()\n",
    "        model.load_state_dict(model_state)\n",
    "        \n",
    "optimizer = get_optimizer(cfg, model, state_dict=optimizer_state)\n",
    "scheduler = get_lr_scheduler(cfg, optimizer, state_dict=scheduler_state)\n",
    "pruner = None\n",
    "if cfg.rigl.dense_allocation is not None:\n",
    "    if cfg.rigl.dense_allocation is not None:\n",
    "        if cfg.model.name == \"skinny_resnet18\":\n",
    "            dense_allocation = (\n",
    "                cfg.rigl.dense_allocation * cfg.model.sparsity_scale_factor\n",
    "            )\n",
    "            print(\n",
    "                f\"Scaling {cfg.rigl.dense_allocation} by \"\n",
    "                f\"{cfg.model.sparsity_scale_factor:.2f} for SkinnyResNet18 \"\n",
    "                f\"New Dense Alloc == {dense_allocation:.6f}\"\n",
    "            )\n",
    "        else:\n",
    "            dense_allocation = cfg.rigl.dense_allocation\n",
    "        T_end = get_T_end(cfg, [0 for _ in range(0,1251)])\n",
    "        if cfg.rigl.const_fan_in:\n",
    "            rigl_scheduler = RigLConstFanScheduler\n",
    "        else:\n",
    "            rigl_scheduler = RigLScheduler\n",
    "        pruner = rigl_scheduler(\n",
    "            model,\n",
    "            optimizer,\n",
    "            dense_allocation=cfg.rigl.dense_allocation,\n",
    "            alpha=cfg.rigl.alpha,\n",
    "            delta=cfg.rigl.delta,\n",
    "            static_topo=cfg.rigl.static_topo,\n",
    "            T_end=T_end,\n",
    "            ignore_linear_layers=cfg.rigl.ignore_linear_layers,\n",
    "            grad_accumulation_n=cfg.rigl.grad_accumulation_n,\n",
    "            sparsity_distribution=cfg.rigl.sparsity_distribution,\n",
    "            erk_power_scale=cfg.rigl.erk_power_scale,\n",
    "            state_dict=pruner_state,\n",
    "            filter_ablation_threshold=cfg.rigl.filter_ablation_threshold,\n",
    "            static_ablation=cfg.rigl.static_ablation,\n",
    "            dynamic_ablation=cfg.rigl.dynamic_ablation,\n",
    "            min_salient_weights_per_neuron=cfg.rigl.min_salient_weights_per_neuron,  # noqa\n",
    "            use_sparse_init=cfg.rigl.use_sparse_initialization,\n",
    "            init_method_str=cfg.rigl.init_method_str,\n",
    "            use_sparse_const_fan_in_for_ablation=cfg.rigl.use_sparse_const_fan_in_for_ablation,  # noqa\n",
    "            initialize_grown_weights=cfg.rigl.initialize_grown_weights,\n",
    "            no_ablation_module_names=cfg.model.no_ablation_module_names\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    cfg,\n",
    "    model,\n",
    "    device,\n",
    "    train_loader,\n",
    "    optimizer,\n",
    "    epoch,\n",
    "    pruner,\n",
    "    step,\n",
    "    logger,\n",
    "    rank,\n",
    "):\n",
    "    model.train()\n",
    "    steps_to_accumulate_grad = get_steps_to_accumulate_grad(\n",
    "        cfg.training.simulated_batch_size, cfg.training.batch_size\n",
    "    )\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        apply_grads = (\n",
    "            True\n",
    "            if steps_to_accumulate_grad == 1\n",
    "            or (\n",
    "                batch_idx != 0\n",
    "                and (batch_idx + 1) % steps_to_accumulate_grad == 0\n",
    "            )\n",
    "            else False\n",
    "        )\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [\n",
    "            {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in t.items()\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        logger.info(loss_dict)\n",
    "        logger.info(loss)\n",
    "        return\n",
    "\n",
    "        # Normalize loss for accumulated grad\n",
    "        loss = loss / steps_to_accumulate_grad\n",
    "\n",
    "        # Will call backwards hooks on model and accumulate dense grads if\n",
    "        # within cfg.rigl.grad_accumulation_n mini-batch steps from update\n",
    "        loss.backward()\n",
    "\n",
    "        if apply_grads:  # If we apply grads, check for topology update and log\n",
    "            if cfg.training.clip_grad_norm is not None:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=cfg.training.clip_grad_norm\n",
    "                )\n",
    "            step += 1\n",
    "            optimizer.step()\n",
    "            if pruner is not None:\n",
    "                # pruner.__call__ returns False if rigl step taken\n",
    "                pruner_called = not pruner()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            if step % cfg.training.log_interval == 0 and rank == 0:\n",
    "                world_size = (\n",
    "                    1\n",
    "                    if cfg.compute.distributed is False\n",
    "                    else cfg.compute.world_size\n",
    "                )\n",
    "                logger.info(\n",
    "                    \"Step: {} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(  # noqa\n",
    "                        step,\n",
    "                        epoch,\n",
    "                        batch_idx * len(images) * world_size,\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "                wandb_data = {\n",
    "                    \"Training Loss\": loss.item(),\n",
    "                }\n",
    "                if pruner is not None:\n",
    "                    wandb_data[\"ITOP Rate\"] = pruner.itop_rs\n",
    "                    if (\n",
    "                        cfg.wandb.log_filter_stats\n",
    "                        and rank == 0\n",
    "                        and pruner_called\n",
    "                    ):\n",
    "                        # If we updated the pruner\n",
    "                        # log filter-wise statistics to wandb\n",
    "                        pruner.log_meters(step=step)\n",
    "                wandb.log(wandb_data, step=step)\n",
    "\n",
    "            # We zero grads after logging pruner filter meters\n",
    "            optimizer.zero_grad()\n",
    "            if cfg.training.dry_run:\n",
    "                logger.warning(\"Dry run, exiting after one training step\")\n",
    "                return step\n",
    "            if (\n",
    "                cfg.training.max_steps is not None\n",
    "                and step > cfg.training.max_steps\n",
    "            ):\n",
    "                return step\n",
    "    return step\n",
    "\n",
    "\n",
    "def test(  # TODO\n",
    "    cfg, model, device, test_loader, epoch, step, rank, logger, training_meter\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    top_k_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            test_loss += F.cross_entropy(\n",
    "                logits,\n",
    "                target,\n",
    "                label_smoothing=cfg.training.label_smoothing,\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "            pred = logits.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "            if cfg.dataset.name == \"imagenet\":\n",
    "                _, top_5_indices = torch.topk(logits, k=5, dim=1, largest=True)\n",
    "                top_5_pred = (\n",
    "                    target.reshape(-1, 1).expand_as(top_5_indices)\n",
    "                    == top_5_indices\n",
    "                ).any(dim=1)\n",
    "                top_k_correct += top_5_pred.sum()\n",
    "            else:\n",
    "                top_k_correct = None\n",
    "    if cfg.compute.distributed:\n",
    "        dist.all_reduce(test_loss, dist.ReduceOp.AVG, async_op=False)\n",
    "        dist.all_reduce(correct, dist.ReduceOp.SUM, async_op=False)\n",
    "        if cfg.dataset.name == \"imagenet\":\n",
    "            dist.all_reduce(top_k_correct, dist.ReduceOp.SUM, async_op=False)\n",
    "            top_k_correct = top_k_correct / len(test_loader.dataset)\n",
    "    training_meter.accuracy = (correct / len(test_loader.dataset)).item()\n",
    "    if rank == 0:\n",
    "        logger.info(\n",
    "            (\n",
    "                \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\"\n",
    "            ).format(\n",
    "                test_loss,\n",
    "                correct,\n",
    "                len(test_loader.dataset),\n",
    "                100.0 * correct / len(test_loader.dataset),\n",
    "            )\n",
    "        )\n",
    "    return test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RigLScheduler(\n",
      "layers=74,\n",
      "nonzero_params=[9408/9408, 4096/4096, 15744/36864, 16384/16384, 16384/16384, 16384/16384, 15744/36864, 16384/16384, 16384/16384, 15744/36864, 16384/16384, 32768/32768, 30720/147456, 65536/65536, 90112/131072, 65536/65536, 30720/147456, 65536/65536, 65536/65536, 30720/147456, 65536/65536, 65536/65536, 30720/147456, 65536/65536, 90368/131072, 60928/589824, 150528/262144, 180224/524288, 150784/262144, 60928/589824, 150528/262144, 150784/262144, 60928/589824, 150528/262144, 150784/262144, 60928/589824, 150528/262144, 150784/262144, 60928/589824, 150528/262144, 150784/262144, 60928/589824, 150528/262144, 180736/524288, 120832/2359296, 301056/1048576, 360448/2097152, 301056/1048576, 120832/2359296, 301056/1048576, 301056/1048576, 120832/2359296, 301056/1048576, 60416/65536, 90368/131072, 150784/262144, 271360/524288, 60928/589824, 60928/589824, 60928/589824, 60928/589824, 60928/589824, 768/768, 3072/3072, 1596416/12845056, 240640/1048576, 93184/93184, 163072/372736, 60928/589824, 60928/589824, 60928/589824, 60928/589824, 60672/262144, 23296/23296],\n",
      "nonzero_percentages=[100.00%, 100.00%, 42.71%, 100.00%, 100.00%, 100.00%, 42.71%, 100.00%, 100.00%, 42.71%, 100.00%, 100.00%, 20.83%, 100.00%, 68.75%, 100.00%, 20.83%, 100.00%, 100.00%, 20.83%, 100.00%, 100.00%, 20.83%, 100.00%, 68.95%, 10.33%, 57.42%, 34.38%, 57.52%, 10.33%, 57.42%, 57.52%, 10.33%, 57.42%, 57.52%, 10.33%, 57.42%, 57.52%, 10.33%, 57.42%, 57.52%, 10.33%, 57.42%, 34.47%, 5.12%, 28.71%, 17.19%, 28.71%, 5.12%, 28.71%, 28.71%, 5.12%, 28.71%, 92.19%, 68.95%, 57.52%, 51.76%, 10.33%, 10.33%, 10.33%, 10.33%, 10.33%, 100.00%, 100.00%, 12.43%, 22.95%, 100.00%, 43.75%, 10.33%, 10.33%, 10.33%, 10.33%, 23.14%, 100.00%],\n",
      "total_nonzero_params=8868160/44395200 (19.98%),\n",
      "total_CONV_nonzero_params=6774848/30035648 (22.56%),\n",
      "step=0,\n",
      "num_rigl_steps=0,\n",
      "ignoring_linear_layers=False,\n",
      "sparsity_distribution=erk,\n",
      "ITOP rate=0.1998,\n",
      "Active Neuron Count=[(64, 64), (64, 64), (64, 64), (256, 256), (256, 256), (64, 64), (64, 64), (256, 256), (64, 64), (64, 64), (256, 256), (128, 128), (128, 128), (512, 512), (512, 512), (128, 128), (128, 128), (512, 512), (128, 128), (128, 128), (512, 512), (128, 128), (128, 128), (512, 512), (256, 256), (256, 256), (1024, 1024), (1024, 1024), (256, 256), (256, 256), (1024, 1024), (256, 256), (256, 256), (1024, 1024), (256, 256), (256, 256), (1024, 1024), (256, 256), (256, 256), (1024, 1024), (256, 256), (256, 256), (1024, 1024), (512, 512), (512, 512), (2048, 2048), (2048, 2048), (512, 512), (512, 512), (2048, 2048), (512, 512), (512, 512), (2048, 2048), (256, 256), (256, 256), (256, 256), (256, 256), (256, 256), (256, 256), (256, 256), (256, 256), (256, 256), (3, 3), (12, 12), (1024, 1024), (1024, 1024), (91, 91), (364, 364), (256, 256), (256, 256), (256, 256), (256, 256), (256, 256), (91, 91)],\n",
      "constant fan ins=[147, 64, 246, 64, 64, 256, 246, 64, 256, 246, 64, 256, 240, 128, 176, 512, 240, 128, 512, 240, 128, 512, 240, 128, 353, 238, 147, 176, 589, 238, 147, 589, 238, 147, 589, 238, 147, 589, 238, 147, 589, 238, 147, 353, 236, 147, 176, 588, 236, 147, 588, 236, 147, 236, 353, 589, 1060, 238, 238, 238, 238, 238, 256, 256, 1559, 235, 1024, 448, 238, 238, 238, 238, 237, 256]\n",
      "Neurons Statically Ablated per layer = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Neurons Dynamically Ablated per layer = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      ")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.77 GiB total capacity; 9.83 GiB already allocated; 1.43 GiB free; 9.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mcompute\u001b[39m.\u001b[39mdistributed:\n\u001b[1;32m     41\u001b[0m     train_loader\u001b[39m.\u001b[39msampler\u001b[39m.\u001b[39mset_epoch(epoch)\n\u001b[0;32m---> 42\u001b[0m step \u001b[39m=\u001b[39m train(\n\u001b[1;32m     43\u001b[0m     cfg,\n\u001b[1;32m     44\u001b[0m     model,\n\u001b[1;32m     45\u001b[0m     device,\n\u001b[1;32m     46\u001b[0m     train_loader,\n\u001b[1;32m     47\u001b[0m     optimizer,\n\u001b[1;32m     48\u001b[0m     epoch,\n\u001b[1;32m     49\u001b[0m     pruner\u001b[39m=\u001b[39;49mpruner,\n\u001b[1;32m     50\u001b[0m     step\u001b[39m=\u001b[39;49mstep,\n\u001b[1;32m     51\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m     52\u001b[0m     rank\u001b[39m=\u001b[39;49mrank,\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     55\u001b[0m loss, box_mAP, mask_mAP \u001b[39m=\u001b[39m test(\n\u001b[1;32m     56\u001b[0m     cfg,\n\u001b[1;32m     57\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     segmentation_meter,\n\u001b[1;32m     65\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg, model, device, train_loader, optimizer, epoch, pruner, step, logger, rank)\u001b[0m\n\u001b[1;32m     27\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[1;32m     28\u001b[0m targets \u001b[39m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     {\n\u001b[1;32m     30\u001b[0m         k: v\u001b[39m.\u001b[39mto(device) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets\n\u001b[1;32m     34\u001b[0m ]\n\u001b[0;32m---> 35\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     36\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     37\u001b[0m logger\u001b[39m.\u001b[39minfo(loss_dict)\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[1;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     99\u001b[0m             )\n\u001b[0;32m--> 101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/models/detection/backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[0;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[1;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(x)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(x)\n\u001b[1;32m    160\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/condensed-sparsity/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.77 GiB total capacity; 9.83 GiB already allocated; 1.43 GiB free; 9.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "run = None  # No WANDB for us here\n",
    "segmentation_meter = SegmentationMeter()\n",
    "if not cfg.experiment.resume_from_checkpoint:\n",
    "    step = 0\n",
    "    if rank == 0:\n",
    "        if run is None:\n",
    "            run_id = datetime.now().strftime(\"%h-%m-%d-%H-%M\")\n",
    "        else:\n",
    "            run_id = run.id\n",
    "        checkpoint = Checkpoint(\n",
    "            run_id=run_id,\n",
    "            cfg=cfg,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            pruner=pruner,\n",
    "            epoch=0,\n",
    "            step=step,\n",
    "            parent_dir=cfg.paths.checkpoints,\n",
    "        )\n",
    "        if (pruner is not None) and (cfg.wandb.log_filter_stats):\n",
    "            # Log inital filter stats before pruning\n",
    "            pruner.log_meters(step=step)\n",
    "\n",
    "    epoch_start = 1\n",
    "else:  # Resuming from checkpoint\n",
    "    checkpoint.model = model\n",
    "    checkpoint.optimizer = optimizer\n",
    "    checkpoint.scheduler = scheduler\n",
    "    checkpoint.pruner = pruner\n",
    "    # Start at the next epoch after the last that successfully was saved\n",
    "    epoch_start = checkpoint.epoch + 1\n",
    "    step = checkpoint.step\n",
    "    # NOTE: we will use acc for checkpointing but this will hold mask_mAP\n",
    "    segmentation_meter._max_mask_mAP = checkpoint.best_acc\n",
    "\n",
    "for epoch in range(epoch_start, cfg.training.epochs + 1):\n",
    "    if pruner is not None and rank == 0:\n",
    "        logger.info(pruner)\n",
    "    if cfg.compute.distributed:\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "    step = train(\n",
    "        cfg,\n",
    "        model,\n",
    "        device,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        epoch,\n",
    "        pruner=pruner,\n",
    "        step=step,\n",
    "        logger=logger,\n",
    "        rank=rank,\n",
    "    )\n",
    "    break\n",
    "    loss, box_mAP, mask_mAP = test(\n",
    "        cfg,\n",
    "        model,\n",
    "        device,\n",
    "        test_loader,\n",
    "        epoch,\n",
    "        step,\n",
    "        rank,\n",
    "        logger,\n",
    "        segmentation_meter,\n",
    "    )\n",
    "    if rank == 0:\n",
    "        wandb.log({\"Learning Rate\": scheduler.get_last_lr()[0]}, step=step)\n",
    "        logger.info(f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "        checkpoint.current_acc = mask_mAP\n",
    "        checkpoint.step = step\n",
    "        checkpoint.epoch = epoch\n",
    "        checkpoint.save_checkpoint()\n",
    "    if cfg.training.dry_run:\n",
    "        break\n",
    "    if cfg.training.max_steps is not None and step > cfg.training.max_steps:\n",
    "        break\n",
    "    scheduler.step()\n",
    "\n",
    "if cfg.training.save_model and rank == 0:\n",
    "    save_path = pathlib.Path(cfg.paths.artifacts)\n",
    "    if not save_path.is_dir():\n",
    "        save_path.mkdir()\n",
    "    f_path = save_path / f\"{cfg.experiment.name}.pt\"\n",
    "    torch.save(model.state_dict(), f_path)\n",
    "    art = wandb.Artifact(name=cfg.experiment.name, type=\"model\")\n",
    "    art.add_file(f_path)\n",
    "    logging.info(f\"artifact path: {f_path}\")\n",
    "    wandb.log_artifact(art)\n",
    "if rank == 0 and cfg.wandb.log_to_wandb:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
