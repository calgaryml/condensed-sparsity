{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "import dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.models import ModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mod(run_id: str, device):\n",
    "    with initialize(\"../configs\", version_base=\"1.2.0\"):\n",
    "        cfg = compose(\n",
    "            \"config.yaml\",\n",
    "            overrides=[\n",
    "                \"compute.distributed=False\",\n",
    "                \"dataset=imagenet\",\n",
    "                \"model=vit\",\n",
    "                f\"experiment.run_id={run_id}\",\n",
    "                \"training.batch_size=2\",\n",
    "            ],\n",
    "        )\n",
    "    dotenv.load_dotenv(\"../.env\", override=True)\n",
    "    os.environ[\"IMAGE_NET_PATH\"]\n",
    "    checkpoint_dir = pathlib.Path(f\"../artifacts/checkpoints/20230601_{run_id}\")\n",
    "    checkpoint = Checkpoint.load_best_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "    model_state = checkpoint.model\n",
    "    model = ModelFactory.load_model(\n",
    "        model=cfg.model.name, dataset=cfg.dataset.name, diet=cfg.rigl.diet\n",
    "    )\n",
    "    model.to(device)\n",
    "    try:\n",
    "        model.load_state_dict(model_state)\n",
    "    except RuntimeError:\n",
    "        model_state = (\n",
    "            checkpoint.get_single_process_model_state_from_distributed_state()\n",
    "        )\n",
    "        model.load_state_dict(model_state)\n",
    "    return model.get_submodule(\"encoder.layers.encoder_layer_11.mlp.0\")\n",
    "\n",
    "\n",
    "__RUN_IDS = {90: \"nrblbn15\"}\n",
    "\n",
    "# t_fc = get_mod(__RUN_IDS[90], \"cpu\") # Run me if you have the artifact on this device\n",
    "\n",
    "with open(\"../artifacts/trained_vit_layers/vit16-mlp-layer-90-torch.pkl\", \"rb\") as handle:\n",
    "    t_fc = torch.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_fc.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from jax import random, vmap, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel = t_fc.weight.detach().cpu().numpy()\n",
    "    print(kernel.shape)\n",
    "    bias = t_fc.bias.detach().cpu().numpy()\n",
    "\n",
    "    # [outC, inC] -> [inC, outC]\n",
    "    kernel = jnp.transpose(kernel, (1, 0))\n",
    "\n",
    "    key = random.key(0)\n",
    "    x = random.normal(key, (64, t_fc.in_features))\n",
    "\n",
    "    variables = {'params': {'kernel': kernel, 'bias': bias}}\n",
    "    j_fc = nn.Dense(features=t_fc.out_features)\n",
    "    j_out = j_fc.apply(variables, x)\n",
    "\n",
    "    t_x = torch.from_numpy(np.array(x))\n",
    "    t_out = t_fc(t_x)\n",
    "    t_out = t_out.detach().cpu().numpy()\n",
    "\n",
    "    np.testing.assert_almost_equal(j_out, t_out, decimal=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../artifacts/trained_vit_layers/vit16-mlp-layer-90-torch.pkl\", \"wb\") as handle:\n",
    "#     torch.save(t_fc, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(variables[\"params\"][\"kernel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(j_fc.param_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables[\"params\"][\"kernel\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1145)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t_fc.weight.sum(dim=1) !=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1145, dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = variables[\"params\"][\"kernel\"]\n",
    "(weights.sum(axis=0)!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 3072)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape  # features, num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1145, dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((weights[:, weights.sum(axis=0)!=0 ]).sum(axis=0)!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1145)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(weights[:, weights.sum(axis=0)!=0 ]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import DTypeLike\n",
    "from jax.typing import ArrayLike \n",
    "from flax.core.scope import VariableDict\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def _torch_get_active_neuron_idx(weight: torch.Tensor) -> torch.Tensor:\n",
    "    # We find all-zero rows in first dimension of weight tensor\n",
    "    return weight.sum(dim=list(range(1, weight.dim()))) != 0\n",
    "\n",
    "\n",
    "def _torch_get_fine_grained_idx(\n",
    "    weight: torch.Tensor, active_neuron_idx\n",
    ") -> torch.Tensor:\n",
    "    return (weight[active_neuron_idx] != 0).to(torch.bool)\n",
    "\n",
    "\n",
    "\n",
    "def _get_active_neuron_idx(kernel: ArrayLike) -> jax.Array:\n",
    "    # We find all-zero rows in first dimension of weight tensor\n",
    "    # NOTE: Only works with fc for now, need to test conv later\n",
    "    # return weight.sum(dim=list(range(1, weight.dim()))) != 0\n",
    "    return kernel.sum(axis=0)!=0  # we swap dim with torch\n",
    "\n",
    "\n",
    "def _get_fine_grained_idx(\n",
    "    kernel: ArrayLike, active_neuron_idx: ArrayLike\n",
    ") -> jax.Array:\n",
    "    return (kernel[:, active_neuron_idx] != 0).astype(\"bool\")\n",
    "\n",
    "\n",
    "kernel, bias = variables[\"params\"][\"kernel\"], variables[\"params\"][\"bias\"]\n",
    "active_neuron_idx = _get_active_neuron_idx(kernel)\n",
    "fine_grained_idx = _get_fine_grained_idx(kernel, active_neuron_idx)\n",
    "\n",
    "t_ani = _torch_get_active_neuron_idx(t_fc.weight)\n",
    "t_fgi = _torch_get_fine_grained_idx(t_fc.weight, t_ani)\n",
    "\n",
    "\n",
    "\n",
    "assert (active_neuron_idx == t_ani.numpy()).all()\n",
    "assert (fine_grained_idx == t_fgi.T.numpy()).all()  # NOTE: transpose here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "struc_kernel = kernel[:, active_neuron_idx]\n",
    "struc_kernel[fine_grained_idx].shape\n",
    "condensed_kernel = struc_kernel[fine_grained_idx].reshape(-1, struc_kernel.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([  0,   0,   0, ..., 767, 767, 767], dtype=int32),\n",
       " Array([   8,   14,   21, ..., 1142, 1143, 1144], dtype=int32))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_idx.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[     8,     14,     21, ...,   3058,   3059,   3060],\n",
       "       [  3063,   3064,   3065, ...,  18359,  18360,  18361],\n",
       "       [ 18363,  18364,  18365, ...,  20575,  20576,  20577],\n",
       "       ...,\n",
       "       [867631, 867633, 867635, ..., 871368, 871373, 871380],\n",
       "       [871384, 871385, 871388, ..., 875413, 875414, 875415],\n",
       "       [875417, 875420, 875421, ..., 879357, 879358, 879359]],      dtype=int32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.flatnonzero(fine_grained_idx).reshape(-1, struc_kernel.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1145)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(206, dtype=int32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(fine_grained_idx[:, 0]!=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained_idx[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = jnp.argwhere(fine_grained_idx[:, 0]!=0).flatten()\n",
    "fine_grained_idx[idx, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = []\n",
    "for neuron in fine_grained_idx.T:\n",
    "    idxs.append(jnp.argwhere(neuron!=0).flatten())\n",
    "\n",
    "idx_seqs = jnp.stack(idxs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 1145)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1145)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc_kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  1,   1,   1, ...,   1,   2,   1],\n",
       "       [  4,   2,   2, ...,   2,  34,   2],\n",
       "       [ 17,  17,  16, ...,   4,  42,  18],\n",
       "       ...,\n",
       "       [757, 759, 758, ..., 757, 759, 757],\n",
       "       [759, 761, 764, ..., 762, 764, 759],\n",
       "       [762, 767, 767, ..., 767, 767, 767]], dtype=int32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 1145, 1145)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struc_kernel[idx_seqs].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[   0,    8],\n",
       "       [   0,   14],\n",
       "       [   0,   21],\n",
       "       ...,\n",
       "       [ 767, 1142],\n",
       "       [ 767, 1143],\n",
       "       [ 767, 1144]], dtype=int32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = jnp.argwhere(fine_grained_idx!=0)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThis BatchTracer with object id 140649380001440 was created on line:\n  /tmp/ipykernel_87785/1142911197.py:1 (<lambda>)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m indx_seqs \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mvmap(\u001b[39mlambda\u001b[39;49;00m row: jnp\u001b[39m.\u001b[39;49margwhere(row\u001b[39m!=\u001b[39;49m\u001b[39m0\u001b[39;49m), in_axes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)(fine_grained_idx)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[1;32m/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m indx_seqs \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(\u001b[39mlambda\u001b[39;00m row: jnp\u001b[39m.\u001b[39;49margwhere(row\u001b[39m!=\u001b[39;49m\u001b[39m0\u001b[39;49m), in_axes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)(fine_grained_idx)\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3600\u001b[0m, in \u001b[0;36margwhere\u001b[0;34m(a, size, fill_value)\u001b[0m\n\u001b[1;32m   3581\u001b[0m \u001b[39m@util\u001b[39m\u001b[39m.\u001b[39m_wraps(np\u001b[39m.\u001b[39margwhere,\n\u001b[1;32m   3582\u001b[0m   lax_description\u001b[39m=\u001b[39m_dedent(\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m   3583\u001b[0m \u001b[39m    Because the size of the output of ``argwhere`` is data-dependent, the function is not\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3598\u001b[0m     fill_value: ArrayLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   3599\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[0;32m-> 3600\u001b[0m   result \u001b[39m=\u001b[39m transpose(vstack(nonzero(a, size\u001b[39m=\u001b[39;49msize, fill_value\u001b[39m=\u001b[39;49mfill_value)))\n\u001b[1;32m   3601\u001b[0m   \u001b[39mif\u001b[39;00m ndim(a) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   3602\u001b[0m     \u001b[39mreturn\u001b[39;00m result[:\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreshape(result\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1379\u001b[0m, in \u001b[0;36mnonzero\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1378\u001b[0m   size \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39msum()\n\u001b[0;32m-> 1379\u001b[0m size \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mconcrete_dim_or_error(size,\n\u001b[1;32m   1380\u001b[0m   \u001b[39m\"\u001b[39;49m\u001b[39mThe size argument of jnp.nonzero must be statically specified \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m   1381\u001b[0m   \u001b[39m\"\u001b[39;49m\u001b[39mto use jnp.nonzero within JAX transformations.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1382\u001b[0m \u001b[39mif\u001b[39;00m arr\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1383\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(zeros(size, \u001b[39mint\u001b[39m) \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m arr\u001b[39m.\u001b[39mshape)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/core.py:1410\u001b[0m, in \u001b[0;36mconcrete_or_error\u001b[0;34m(force, val, context)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[39mreturn\u001b[39;00m force(val\u001b[39m.\u001b[39maval\u001b[39m.\u001b[39mval)\n\u001b[1;32m   1409\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1410\u001b[0m     \u001b[39mraise\u001b[39;00m ConcretizationTypeError(val, context)\n\u001b[1;32m   1411\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m   \u001b[39mreturn\u001b[39;00m force(val)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[].\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThis BatchTracer with object id 140649380001440 was created on line:\n  /tmp/ipykernel_87785/1142911197.py:1 (<lambda>)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "indx_seqs = jax.vmap(lambda row: jnp.argwhere(row!=0), in_axes=1)(fine_grained_idx)\n",
    "# struc_kernel[indx_seqs]\n",
    "\n",
    "# indx_seqs = indx_seqs.reshape(-1, struc_kernel.shape[1])\n",
    "# indx_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1145)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[   8,   14,   21, ...,  768,  769,  770],\n",
       "       [ 773,  774,  775, ...,   39,   40,   41],\n",
       "       [  43,   44,   45, ..., 1110, 1111, 1112],\n",
       "       ...,\n",
       "       [ 866,  868,  870, ...,   23,   28,   35],\n",
       "       [  39,   40,   43, ...,  633,  634,  635],\n",
       "       [ 637,  640,  641, ..., 1142, 1143, 1144]], dtype=int32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indx_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1145, 206])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, t_input_mask = t_fgi.nonzero(as_tuple=True)\n",
    "t_input_mask = t_input_mask.reshape(\n",
    "            shape=(t_fc.weight[t_ani].shape[0], -1)\n",
    "        )\n",
    "t_input_mask.shape\n",
    "\n",
    "# assert (t_input_mask.T.numpy() == indx_seqs).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, t_input_mask = self.fine_grained_idx.nonzero(as_tuple=True)\n",
    "self.input_mask = self.input_mask.reshape(\n",
    "    shape=(module.weight[self.active_neuron_idx].shape[0], -1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import DTypeLike\n",
    "from jax.typing import ArrayLike\n",
    "from flax.core.scope import VariableDict\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def condensed_param_converter(dense_params: VariableDict, dtype: Optional[DTypeLike]=None) -> VariableDict:\n",
    "    dense_params = deepcopy(dense_params)\n",
    "    kernel, bias = dense_params[\"params\"][\"kernel\"], dense_params[\"params\"][\"bias\"]\n",
    "    if dtype is None:\n",
    "        dtype = kernel.dtype\n",
    "\n",
    "    \n",
    "    active_neuron_idx = _get_active_neuron_idx(kernel)\n",
    "    fine_grained_idx = _get_fine_grained_idx(kernel, active_neuron_idx)\n",
    "    struct_kernel = kernel[:, active_neuron_idx]\n",
    "    condensed_kernel = struc_kernel[fine_grained_idx].reshape(-1, struct_kernel.shape[1])\n",
    "    idxs = []\n",
    "    for neuron in fine_grained_idx.T:\n",
    "        idxs.append(jnp.argwhere(neuron!=0).flatten())\n",
    "    indx_seqs = jnp.stack(idxs).T\n",
    "    return dict(\n",
    "        params=dict(\n",
    "            kernel=condensed_kernel,\n",
    "            bias=bias[active_neuron_idx],\n",
    "            indx_seqs=indx_seqs\n",
    "        )\n",
    "    )\n",
    "\n",
    "condensed_params = condensed_param_converter(variables)\n",
    "\n",
    "# class CondensedLinear(nn.Module):\n",
    "#     dense_params:\n",
    "#     dtype: Optional[DTypeLike] = None\n",
    "\n",
    "    \n",
    "#     def setup(self):\n",
    "#         if self.dtype is None:\n",
    "#             self.dtype = self.module.param_dtype\n",
    "#         self.active_neuron_idx = self.module.weight.sum(dim=1) != 0\n",
    "#         self.fine_grained_idx = (self.module.weight[self.active_neuron_idx] != 0).to(\n",
    "#             torch.bool\n",
    "#         )\n",
    "#         _, self.input_mask = self.fine_grained_idx.nonzero(as_tuple=True)\n",
    "#         self.input_mask = self.input_mask.reshape(\n",
    "#             shape=(module.weight[self.active_neuron_idx].shape[0], -1)\n",
    "#         )\n",
    "#         with torch.no_grad():\n",
    "#             # self.weight = nn.Parameter(\n",
    "#             #     module.weight[self.active_neuron_idx].contiguous()\n",
    "#             # )\n",
    "#             # self.condensed_weight = nn.Parameter(\n",
    "#             #     self.weight[self.fine_grained_idx]\n",
    "#             #     .reshape(shape=(self.weight.shape[0], -1))\n",
    "#             #     .contiguous()\n",
    "#             # )\n",
    "#             # self.sparse_weight = nn.Parameter(\n",
    "#             #     self.weight.to_sparse_csr()\n",
    "#             # )\n",
    "#             # if hasattr(module, \"bias\"):\n",
    "#             #     self.bias = nn.Parameter(\n",
    "#             #         module.bias[self.active_neuron_idx].contiguous()\n",
    "#             #     )\n",
    "#             # else:\n",
    "#             #     self.register_parameter(\"bias\", None)\n",
    "#             self.weight = nn.Parameter(\n",
    "#                 torch.clone(\n",
    "#                     module.weight[self.active_neuron_idx].detach().type(dtype)\n",
    "#                 )\n",
    "#             )\n",
    "#             self.condensed_weight = nn.Parameter(\n",
    "#                 torch.clone(\n",
    "#                     self.weight[self.fine_grained_idx]\n",
    "#                     .reshape(shape=(self.weight.shape[0], -1))\n",
    "#                     .detach()\n",
    "#                     .type(dtype)\n",
    "#                 ),\n",
    "#                 requires_grad=False,\n",
    "#             )\n",
    "#             self.sparse_weight = nn.Parameter(\n",
    "#                 torch.clone(self.weight.detach().type(dtype).to_sparse_csr()),\n",
    "#                 requires_grad=False,\n",
    "#             )\n",
    "#             if hasattr(module, \"bias\"):\n",
    "#                 self.bias = nn.Parameter(\n",
    "#                     torch.clone(\n",
    "#                         module.bias[self.active_neuron_idx].detach().type(dtype)\n",
    "#                     ),\n",
    "#                     requires_grad=False,\n",
    "#                 )\n",
    "#             else:\n",
    "#                 self.register_parameter(\"bias\", None)\n",
    "        \n",
    "#     def __call__():\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables[\"params\"][\"bias\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {'params': {'kernel': kernel, 'bias': bias}}\n",
    "j_fc = nn.Dense(features=t_fc.out_features)\n",
    "j_out = j_fc.apply(variables, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 1145)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_params[\"params\"][\"indx_seqs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 768)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "\n",
    "# TODO: Figure out this broadcasting buiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(206, 1145), (64, 1145, 206)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/util.py:263\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 263\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/util.py:256\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    255\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:152\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m@cache\u001b[39m()\n\u001b[1;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_broadcast_shapes_cached\u001b[39m(\u001b[39m*\u001b[39mshapes: \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m--> 152\u001b[0m   \u001b[39mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[39m*\u001b[39;49mshapes)\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(206, 1145), (64, 1145, 206)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb Cell 40\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_orig\u001b[39m(variables, \u001b[39minput\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39msum(variables[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m[:, variables[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mindx_seqs\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mT], axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m variables[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m condensed_out \u001b[39m=\u001b[39m forward_orig(condensed_params, x)\n",
      "\u001b[1;32m/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb Cell 40\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_orig\u001b[39m(variables, \u001b[39minput\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39msum(variables[\u001b[39m\"\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mkernel\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m*\u001b[39;49m \u001b[39minput\u001b[39;49m[:, variables[\u001b[39m\"\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mindx_seqs\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mT], axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m variables[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:256\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    254\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 256\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    257\u001b[0m \u001b[39m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(other) \u001b[39min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:96\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(x1, x2, \u001b[39m/\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m   x1, x2 \u001b[39m=\u001b[39m promote_args(numpy_fn\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, x1, x2)\n\u001b[1;32m     97\u001b[0m   \u001b[39mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[39mif\u001b[39;00m x1\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mbool_ \u001b[39melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:358\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    356\u001b[0m check_arraylike(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    357\u001b[0m _check_no_float0s(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 358\u001b[0m \u001b[39mreturn\u001b[39;00m promote_shapes(fun_name, \u001b[39m*\u001b[39;49mpromote_dtypes(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:247\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mjax_numpy_rank_promotion \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    246\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 247\u001b[0m result_rank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(lax\u001b[39m.\u001b[39;49mbroadcast_shapes(\u001b[39m*\u001b[39;49mshapes))\n\u001b[1;32m    248\u001b[0m \u001b[39mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (result_rank \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(shp)) \u001b[39m+\u001b[39m shp)\n\u001b[1;32m    249\u001b[0m         \u001b[39mfor\u001b[39;00m arg, shp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:168\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    166\u001b[0m result_shape \u001b[39m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(206, 1145), (64, 1145, 206)]"
     ]
    }
   ],
   "source": [
    "# def forward_orig(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray) -> jnp.ndarray:\n",
    "#     return jnp.sum(weights * input[:, indx_seqs], axis=2)\n",
    "\n",
    "def forward_orig(variables, input) -> ArrayLike:\n",
    "    return jnp.sum(variables[\"params\"][\"kernel\"] * input[:, variables[\"params\"][\"indx_seqs\"].T], axis=2) + variables[\"params\"][\"bias\"]\n",
    "\n",
    "condensed_out = forward_orig(condensed_params, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
