{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "import dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.models import ModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mod(run_id: str, device):\n",
    "    with initialize(\"../configs\", version_base=\"1.2.0\"):\n",
    "        cfg = compose(\n",
    "            \"config.yaml\",\n",
    "            overrides=[\n",
    "                \"compute.distributed=False\",\n",
    "                \"dataset=imagenet\",\n",
    "                \"model=vit\",\n",
    "                f\"experiment.run_id={run_id}\",\n",
    "                \"training.batch_size=2\",\n",
    "            ],\n",
    "        )\n",
    "    dotenv.load_dotenv(\"../.env\", override=True)\n",
    "    os.environ[\"IMAGE_NET_PATH\"]\n",
    "    checkpoint_dir = pathlib.Path(f\"../artifacts/checkpoints/20230601_{run_id}\")\n",
    "    checkpoint = Checkpoint.load_best_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "    model_state = checkpoint.model\n",
    "    model = ModelFactory.load_model(\n",
    "        model=cfg.model.name, dataset=cfg.dataset.name, diet=cfg.rigl.diet\n",
    "    )\n",
    "    model.to(device)\n",
    "    try:\n",
    "        model.load_state_dict(model_state)\n",
    "    except RuntimeError:\n",
    "        model_state = (\n",
    "            checkpoint.get_single_process_model_state_from_distributed_state()\n",
    "        )\n",
    "        model.load_state_dict(model_state)\n",
    "    return model.get_submodule(\"encoder.layers.encoder_layer_11.mlp.0\")\n",
    "\n",
    "\n",
    "__RUN_IDS = {90: \"nrblbn15\"}\n",
    "\n",
    "# t_fc = get_mod(__RUN_IDS[90], \"cpu\") # Run me if you have the artifact on this device\n",
    "\n",
    "with open(\"../artifacts/trained_vit_layers/vit16-mlp-layer-90-torch.pkl\", \"rb\") as handle:\n",
    "    t_fc = torch.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 768)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from jax import random, vmap, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    kernel = t_fc.weight.detach().cpu().numpy()\n",
    "    print(kernel.shape)\n",
    "    bias = t_fc.bias.detach().cpu().numpy()\n",
    "\n",
    "    # [outC, inC] -> [inC, outC]\n",
    "    kernel = jnp.transpose(kernel, (1, 0))\n",
    "\n",
    "    key = random.key(0)\n",
    "    x = random.normal(key, (64, t_fc.in_features))\n",
    "\n",
    "    variables = {'params': {'kernel': kernel, 'bias': bias}}\n",
    "    j_fc = nn.Dense(features=t_fc.out_features)\n",
    "    j_out = j_fc.apply(variables, x)\n",
    "\n",
    "    t_x = torch.from_numpy(np.array(x))\n",
    "    t_out = t_fc(t_x)\n",
    "    t_out = t_out.detach().cpu().numpy()\n",
    "\n",
    "    np.testing.assert_almost_equal(j_out, t_out, decimal=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../artifacts/trained_vit_layers/vit16-mlp-layer-90-torch.pkl\", \"wb\") as handle:\n",
    "#     torch.save(t_fc, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import DTypeLike\n",
    "from jax.typing import ArrayLike \n",
    "from flax.core.scope import VariableDict\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def _torch_get_active_neuron_idx(weight: torch.Tensor) -> torch.Tensor:\n",
    "    # We find all-zero rows in first dimension of weight tensor\n",
    "    return weight.sum(dim=list(range(1, weight.dim()))) != 0\n",
    "\n",
    "\n",
    "def _torch_get_fine_grained_idx(\n",
    "    weight: torch.Tensor, active_neuron_idx\n",
    ") -> torch.Tensor:\n",
    "    return (weight[active_neuron_idx] != 0).to(torch.bool)\n",
    "\n",
    "\n",
    "\n",
    "def _get_active_neuron_idx(kernel: ArrayLike) -> jax.Array:\n",
    "    # We find all-zero rows in first dimension of weight tensor\n",
    "    # NOTE: Only works with fc for now, need to test conv later\n",
    "    # return weight.sum(dim=list(range(1, weight.dim()))) != 0\n",
    "    # return kernel.sum(axis=0)!=0  # we swap dim with torch\n",
    "    return kernel.sum(axis=1)!=0  # we swap dim with torch\n",
    "\n",
    "\n",
    "def _get_fine_grained_idx(\n",
    "    kernel: ArrayLike, active_neuron_idx: ArrayLike\n",
    ") -> jax.Array:\n",
    "    return (kernel[active_neuron_idx] != 0).astype(\"bool\")\n",
    "\n",
    "\n",
    "kernel, bias = variables[\"params\"][\"kernel\"].T, variables[\"params\"][\"bias\"].T\n",
    "active_neuron_idx = _get_active_neuron_idx(kernel)\n",
    "fine_grained_idx = _get_fine_grained_idx(kernel, active_neuron_idx)\n",
    "\n",
    "t_ani = _torch_get_active_neuron_idx(t_fc.weight)\n",
    "t_fgi = _torch_get_fine_grained_idx(t_fc.weight, t_ani)\n",
    "\n",
    "\n",
    "\n",
    "assert (active_neuron_idx == t_ani.numpy()).all()\n",
    "assert (fine_grained_idx == t_fgi.numpy()).all()  # NOTE: transpose here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# struc_kernel = kernel[:, active_neuron_idx]\n",
    "# struc_kernel[fine_grained_idx].shape\n",
    "# condensed_kernel = struc_kernel[fine_grained_idx].reshape(-1, struc_kernel.shape[1])\n",
    "struc_kernel = kernel[active_neuron_idx]\n",
    "struc_kernel[fine_grained_idx].shape\n",
    "condensed_kernel = struc_kernel[fine_grained_idx].reshape(struc_kernel.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([   0,    0,    0, ..., 1144, 1144, 1144], dtype=int32),\n",
       " Array([  1,   4,  17, ..., 757, 759, 767], dtype=int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_idx.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.typing import DTypeLike\n",
    "from jax.typing import ArrayLike\n",
    "from flax.core.scope import VariableDict\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def condensed_param_converter(dense_params: VariableDict, dtype: Optional[DTypeLike]=None) -> VariableDict:\n",
    "    dense_params = deepcopy(dense_params)\n",
    "    kernel, bias = dense_params[\"params\"][\"kernel\"].T, dense_params[\"params\"][\"bias\"].T\n",
    "    # Without transpose here I found broadcasting issues in original condensed implementation\n",
    "    if dtype is None:\n",
    "        dtype = kernel.dtype\n",
    "\n",
    "    \n",
    "    active_neuron_idx = _get_active_neuron_idx(kernel)\n",
    "    fine_grained_idx = _get_fine_grained_idx(kernel, active_neuron_idx)\n",
    "    struct_kernel = kernel[active_neuron_idx]\n",
    "    condensed_kernel = struc_kernel[fine_grained_idx].reshape(struct_kernel.shape[0], -1)\n",
    "    # struct_kernel = kernel[:, active_neuron_idx]\n",
    "    # condensed_kernel = struc_kernel[fine_grained_idx].reshape(-1, struct_kernel.shape[1])\n",
    "    \n",
    "    # TODO: Can speed-up the below, we used torch.nonzero(as_tuple=True) prev\n",
    "    idxs = []\n",
    "    for neuron in fine_grained_idx:\n",
    "        idxs.append(jnp.argwhere(neuron!=0).flatten())\n",
    "    indx_seqs = jnp.stack(idxs)\n",
    "    return dict(\n",
    "        params=dict(\n",
    "            kernel=condensed_kernel,\n",
    "            bias=bias[active_neuron_idx],\n",
    "            indx_seqs=indx_seqs\n",
    "        )\n",
    "    )\n",
    "\n",
    "condensed_params = condensed_param_converter(variables)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondensedLinear(nn.Module):\n",
    "    features: int\n",
    "    fan_in: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input: ArrayLike) -> jax.Array:\n",
    "        kernel = self.param(\"kernel\", self.kernel_init, (self.features, self.fan_in))\n",
    "        bias = self.param(\"bias\", self.bias_init, (self.features,))\n",
    "        indx_seqs = self.param(\"indx_seqs\", self.kernel_init, (self.features, self.fan_in))\n",
    "        return jnp.sum(kernel * input[:, indx_seqs], axis=2) + bias\n",
    "    \n",
    "\n",
    "class CondensedLinearWithState(nn.Module):\n",
    "    kernel: ArrayLike\n",
    "    bias: ArrayLike\n",
    "    indx_seqs: ArrayLike\n",
    "        \n",
    "    def __call__(self, input: ArrayLike) -> jax.Array:\n",
    "        return jnp.sum(self.kernel * input[:, self.indx_seqs], axis=2) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.06890744, -0.1435932 ,  0.18185034, ...,  0.19422379,\n",
       "        -1.0179393 , -0.8103184 ],\n",
       "       [-0.5491562 , -0.02691372, -0.9410149 , ...,  0.36850947,\n",
       "         0.23354463, -0.14654508],\n",
       "       [ 0.13191406, -0.25492084,  0.6769124 , ...,  0.5262484 ,\n",
       "        -0.12328374, -0.06665101],\n",
       "       ...,\n",
       "       [ 0.78998613, -0.04756172,  0.49412474, ...,  0.3047766 ,\n",
       "        -0.10613711,  1.3015689 ],\n",
       "       [ 0.5847777 ,  0.41667846, -0.80086994, ...,  0.18786182,\n",
       "         0.54880977, -0.97745925],\n",
       "       [ 0.6309595 , -0.260169  , -1.5061429 , ..., -0.5892743 ,\n",
       "         0.8418522 , -0.12319788]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return jnp.sum(variables[\"params\"][\"kernel\"] * input[:, variables[\"params\"][\"indx_seqs\"]], axis=2) + variables[\"params\"][\"bias\"]\n",
    "cl = CondensedLinear(features=1145, fan_in=206)\n",
    "cl_fast = jax.jit(lambda x: cl.apply(condensed_params, x))\n",
    "cl_fast(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 µs ± 7 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "cl_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 2.2639807e-12,  6.8915978e-02,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -8.1014007e-01, -8.9613508e-12],\n",
       "       [ 2.2639807e-12, -5.4868144e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -1.4640963e-01, -8.9613508e-12],\n",
       "       [ 2.2639807e-12,  1.3192402e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -6.6684671e-02, -8.9613508e-12],\n",
       "       ...,\n",
       "       [ 2.2639807e-12,  7.9031599e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12,  1.3017950e+00, -8.9613508e-12],\n",
       "       [ 2.2639807e-12,  5.8474576e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -9.7734028e-01, -8.9613508e-12],\n",
       "       [ 2.2639807e-12,  6.3065708e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -1.2312210e-01, -8.9613508e-12]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_fc_fast = jax.jit(lambda x: j_fc.apply(variables, x))\n",
    "j_fc_fast(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.8 µs ± 1.51 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "j_fc_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.06890744, -0.1435932 ,  0.18185034, ...,  0.19422379,\n",
       "        -1.0179393 , -0.8103184 ],\n",
       "       [-0.5491562 , -0.02691372, -0.9410149 , ...,  0.36850947,\n",
       "         0.23354463, -0.14654508],\n",
       "       [ 0.13191406, -0.25492084,  0.6769124 , ...,  0.5262484 ,\n",
       "        -0.12328374, -0.06665101],\n",
       "       ...,\n",
       "       [ 0.78998613, -0.04756172,  0.49412474, ...,  0.3047766 ,\n",
       "        -0.10613711,  1.3015689 ],\n",
       "       [ 0.5847777 ,  0.41667846, -0.80086994, ...,  0.18786182,\n",
       "         0.54880977, -0.97745925],\n",
       "       [ 0.6309595 , -0.260169  , -1.5061429 , ..., -0.5892743 ,\n",
       "         0.8418522 , -0.12319788]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_2 = CondensedLinearWithState(**condensed_params[\"params\"])\n",
    "cl_2_fast = jax.jit(lambda x: cl_2(x))\n",
    "cl_2_fast(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 µs ± 5.65 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cl_2_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables[\"params\"][\"bias\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (3072, 768) but got shape (768, 3072) instead for parameter \"kernel\" in \"/\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m variables \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m'\u001b[39m: kernel, \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m: bias}}\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m j_fc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDense(features\u001b[39m=\u001b[39mt_fc\u001b[39m.\u001b[39mout_features)\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d696b652f636f6e64656e7365642d7370617273697479222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d696e6e6577616e6b61227d2c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f6d696b652f636f6e64656e7365642d73706172736974792f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/home/user/condensed-sparsity/notebooks/jax_benchmarks.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m j_out \u001b[39m=\u001b[39m j_fc\u001b[39m.\u001b[39;49mapply(variables, x)\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/flax/linen/linear.py:234\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39m@compact\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m    226\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m    The transformed input.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m   kernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam(\n\u001b[1;32m    235\u001b[0m       \u001b[39m'\u001b[39;49m\u001b[39mkernel\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    236\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_init,\n\u001b[1;32m    237\u001b[0m       (jnp\u001b[39m.\u001b[39;49mshape(inputs)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures),\n\u001b[1;32m    238\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_dtype,\n\u001b[1;32m    239\u001b[0m   )\n\u001b[1;32m    240\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_bias:\n\u001b[1;32m    241\u001b[0m     bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam(\n\u001b[1;32m    242\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_init, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures,), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_dtype\n\u001b[1;32m    243\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/build/.venv/lib/python3.10/site-packages/flax/core/scope.py:975\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[39mfor\u001b[39;00m val, abs_val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[1;32m    971\u001b[0m     \u001b[39m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[1;32m    972\u001b[0m     \u001b[39m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[1;32m    973\u001b[0m     \u001b[39m# for inference to a half float type for example.\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m jnp\u001b[39m.\u001b[39mshape(val) \u001b[39m!=\u001b[39m jnp\u001b[39m.\u001b[39mshape(abs_val):\n\u001b[0;32m--> 975\u001b[0m       \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mScopeParamShapeError(\n\u001b[1;32m    976\u001b[0m           name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_text, jnp\u001b[39m.\u001b[39mshape(abs_val), jnp\u001b[39m.\u001b[39mshape(val)\n\u001b[1;32m    977\u001b[0m       )\n\u001b[1;32m    978\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_mutable_collection(\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (3072, 768) but got shape (768, 3072) instead for parameter \"kernel\" in \"/\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "variables = {'params': {'kernel': kernel, 'bias': bias}}\n",
    "j_fc = nn.Dense(features=t_fc.out_features)\n",
    "j_out = j_fc.apply(variables, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 206)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_params[\"params\"][\"indx_seqs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 768)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "\n",
    "# TODO: Figure out this broadcasting buiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 768)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 1145)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_params[\"params\"][\"indx_seqs\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[  1,   1,   1, ...,   1,   2,   1],\n",
       "       [  4,   2,   2, ...,   2,  34,   2],\n",
       "       [ 17,  17,  16, ...,   4,  42,  18],\n",
       "       ...,\n",
       "       [757, 759, 758, ..., 757, 759, 757],\n",
       "       [759, 761, 764, ..., 762, 764, 759],\n",
       "       [762, 767, 767, ..., 767, 767, 767]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_params[\"params\"][\"indx_seqs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_fc.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206, 1145)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_params[\"params\"][\"kernel\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 768)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(x, in_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206, 1145)\n",
      "(206, 1145)\n",
      "(64, 768)\n"
     ]
    }
   ],
   "source": [
    "print(condensed_params[\"params\"][\"kernel\"].shape)\n",
    "print(condensed_params[\"params\"][\"indx_seqs\"].shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_orig(variables, input) -> ArrayLike:\n",
    "    return jnp.sum(variables[\"params\"][\"kernel\"] * input[:, variables[\"params\"][\"indx_seqs\"]], axis=2) + variables[\"params\"][\"bias\"]\n",
    "\n",
    "condensed_out = forward_orig(condensed_params, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
