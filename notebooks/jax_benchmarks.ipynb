{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "import dotenv\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from rigl_torch.utils.checkpoint import Checkpoint\n",
    "from rigl_torch.models import ModelFactory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mod(run_id: str, device):\n",
    "    with initialize(\"../configs\", version_base=\"1.2.0\"):\n",
    "        cfg = compose(\n",
    "            \"config.yaml\",\n",
    "            overrides=[\n",
    "                \"compute.distributed=False\",\n",
    "                \"dataset=imagenet\",\n",
    "                \"model=vit\",\n",
    "                f\"experiment.run_id={run_id}\",\n",
    "                \"training.batch_size=2\",\n",
    "            ],\n",
    "        )\n",
    "    dotenv.load_dotenv(\"../.env\", override=True)\n",
    "    os.environ[\"IMAGE_NET_PATH\"]\n",
    "    checkpoint_dir = pathlib.Path(f\"../artifacts/checkpoints/20230601_{run_id}\")\n",
    "    checkpoint = Checkpoint.load_best_checkpoint(checkpoint_dir=checkpoint_dir)\n",
    "    model_state = checkpoint.model\n",
    "    model = ModelFactory.load_model(\n",
    "        model=cfg.model.name, dataset=cfg.dataset.name, diet=cfg.rigl.diet\n",
    "    )\n",
    "    model.to(device)\n",
    "    try:\n",
    "        model.load_state_dict(model_state)\n",
    "    except RuntimeError:\n",
    "        model_state = (\n",
    "            checkpoint.get_single_process_model_state_from_distributed_state()\n",
    "        )\n",
    "        model.load_state_dict(model_state)\n",
    "    return model.get_submodule(\"encoder.layers.encoder_layer_11.mlp.0\")\n",
    "\n",
    "\n",
    "__RUN_IDS = {90: \"nrblbn15\"}\n",
    "\n",
    "# t_fc = get_mod(__RUN_IDS[90], \"cpu\") # Run me if you have the artifact on this device\n",
    "\n",
    "with open(\"../artifacts/trained_vit_layers/vit16-mlp-layer-90-torch.pkl\", \"rb\") as handle:  # TODO: try skinnier layer\n",
    "    t_fc = torch.load(handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1696012059.441441 3998717 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n",
      "2023-09-29 12:27:39.452337: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:276] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-09-29 12:27:39.452809: E external/xla/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 535.86.10 does not match DSO version 535.104.5 -- cannot find working devices in this configuration\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional, Tuple, Union\n",
    "from jax import random, vmap, numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "# _dtype = jnp.bfloat16 # faster on gpu\n",
    "_dtype = jnp.float32 # faster on cpu @ batch size 1. slower at 64\n",
    "# t_fc = t_fc.to(torch.bfloat16) # try bf16, Time to beat (176micro for dense, 137 micro for fastest condensed)\n",
    "# conversion to jax/flax\n",
    "with torch.no_grad():\n",
    "    kernel = t_fc.weight.detach().cpu().numpy()\n",
    "    bias = t_fc.bias.detach().cpu().numpy()\n",
    "\n",
    "    # [outC, inC] -> [inC, outC]\n",
    "    kernel = jnp.transpose(kernel, (1, 0)).astype(_dtype)\n",
    "\n",
    "    key = random.key(0)\n",
    "    x = random.normal(key, (64, t_fc.in_features))\n",
    "\n",
    "    variables = {'params': {'kernel': kernel, 'bias': bias.astype(_dtype)}}\n",
    "    j_fc = nn.Dense(features=t_fc.out_features)\n",
    "    j_out = j_fc.apply(variables, x)\n",
    "\n",
    "    t_x = torch.from_numpy(np.array(x))\n",
    "    t_out = t_fc(t_x)\n",
    "    t_out = t_out.detach().cpu().numpy()\n",
    "\n",
    "    np.testing.assert_almost_equal(j_out, t_out, decimal=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = t_fc.in_features\n",
    "layer_width = t_fc.out_features\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey = random.split(key)\n",
    "x = random.normal(subkey, (batch_size, input_size), dtype=_dtype)\n",
    "x = jax.device_put(x)\n",
    "\n",
    "dense_layer = nn.Dense(features=layer_width, use_bias=True)\n",
    "dense_params = variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.87 ms ± 1.02 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dense_layer.apply(dense_params, x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_fast = jax.jit(lambda x: dense_layer.apply(dense_params, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 2.2639807e-12,  7.5584012e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12,  8.1965402e-02, -8.9613508e-12],\n",
       "       [ 2.2639807e-12, -2.0178687e-02,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12,  1.0754541e+00, -8.9613508e-12],\n",
       "       [ 2.2639807e-12,  7.9683006e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -4.9104637e-01, -8.9613508e-12],\n",
       "       ...,\n",
       "       [ 2.2639807e-12,  3.0003309e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -1.6757858e-01, -8.9613508e-12],\n",
       "       [ 2.2639807e-12, -1.5575090e-02,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12, -5.2756774e-01, -8.9613508e-12],\n",
       "       [ 2.2639807e-12, -2.2859134e-01,  1.9262955e-12, ...,\n",
       "        -1.2273963e-12,  1.3307133e-01, -8.9613508e-12]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 µs ± 7.51 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dense_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072,)\n",
      "(768, 3072)\n",
      "(1145,)\n",
      "(1145, 206)\n",
      "(1145, 206)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'bias': None, 'indx_seqs': None, 'kernel': None}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flax condensed sparsity\n",
    "\n",
    "from numpy.typing import DTypeLike\n",
    "from jax.typing import ArrayLike\n",
    "from flax.core.scope import VariableDict\n",
    "from copy import deepcopy\n",
    "\n",
    "def condensed_param_converter(dense_params: VariableDict, dtype: Optional[DTypeLike]=None) -> VariableDict:\n",
    "    \"\"\"Convert dense tensor with sparse weights into condensed version\"\"\"\n",
    "    dense_params = deepcopy(dense_params)\n",
    "    kernel, bias = dense_params[\"params\"][\"kernel\"].T, dense_params[\"params\"][\"bias\"].T\n",
    "    # Without transpose here I found broadcasting issues in original condensed implementation\n",
    "    if dtype is None:\n",
    "        dtype = kernel.dtype\n",
    "    active_neuron_idx = _get_active_neuron_idx(kernel)\n",
    "    fine_grained_idx = _get_fine_grained_idx(kernel, active_neuron_idx)\n",
    "    struct_kernel = kernel[active_neuron_idx]\n",
    "    condensed_kernel = struct_kernel[fine_grained_idx].reshape(struct_kernel.shape[0], -1)\n",
    "\n",
    "    # TODO: Can speed-up the below, we used torch.nonzero(as_tuple=True)\n",
    "    # previously, need to translate the typical 2D tensor output from jax.nonzero into the same\n",
    "    # format. We don't really care about speed here for our purposes anyways\n",
    "    idxs = []\n",
    "    for neuron in fine_grained_idx:\n",
    "        idxs.append(jnp.argwhere(neuron!=0).flatten())\n",
    "    indx_seqs = jnp.stack(idxs)\n",
    "    return dict(\n",
    "        params=dict(\n",
    "            kernel=condensed_kernel,\n",
    "            bias=bias[active_neuron_idx],\n",
    "            indx_seqs=indx_seqs\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _get_active_neuron_idx(kernel: ArrayLike) -> jax.Array:\n",
    "  # We find all-zero rows in first dimension of weight tensor\n",
    "  return kernel.sum(axis=list(range(1, kernel.ndim))) != 0\n",
    "\n",
    "\n",
    "def _get_fine_grained_idx(\n",
    "    kernel: ArrayLike, active_neuron_idx: ArrayLike\n",
    ") -> jax.Array:\n",
    "    return (kernel[active_neuron_idx] != 0).astype(\"bool\")\n",
    "\n",
    "class CondensedLinear(nn.Module):\n",
    "    features: int\n",
    "    fan_in: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input: ArrayLike) -> jax.Array:\n",
    "        kernel = self.param(\"kernel\", self.kernel_init, (self.features, self.fan_in))\n",
    "        bias = self.param(\"bias\", self.bias_init, (self.features,))\n",
    "        indx_seqs = self.param(\"indx_seqs\", self.kernel_init, (self.features, self.fan_in))\n",
    "        return jnp.sum(kernel * input[:, indx_seqs], axis=2) + bias\n",
    "\n",
    "\n",
    "condensed_params = condensed_param_converter(variables)\n",
    "jax.tree_util.tree_map(lambda x: print(x.shape), variables)\n",
    "jax.tree_util.tree_map(lambda x: print(x.shape), condensed_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1145, 206)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condensed_params[\"params\"][\"kernel\"].shape # features, fan_in for condensed linear ctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.75584006, -0.17063873,  1.6859026 , ...,  0.18504576,\n",
       "        -0.00192568,  0.08196551],\n",
       "       [-0.02017869, -0.71082234,  0.16039723, ...,  0.41155237,\n",
       "        -0.4476214 ,  1.0754542 ],\n",
       "       [ 0.7968303 , -0.10603629,  1.2908362 , ..., -0.32634115,\n",
       "        -0.48441118, -0.49104658],\n",
       "       ...,\n",
       "       [ 0.30003327, -0.17096956,  0.10126591, ...,  0.6488617 ,\n",
       "        -0.1293261 , -0.16757864],\n",
       "       [-0.01557497, -0.39440617,  0.21288827, ...,  0.32918108,\n",
       "        -0.12092257, -0.527568  ],\n",
       "       [-0.22859119, -0.54928684,  0.2176865 , ...,  0.11042877,\n",
       "         0.5686784 ,  0.13307133]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = CondensedLinear(*condensed_params[\"params\"][\"kernel\"].shape)\n",
    "cl_fast = jax.jit(lambda x: cl.apply(condensed_params, x))\n",
    "cl_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8 ms ± 377 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "cl_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key, subkey = random.split(key)\n",
    "# input = random.uniform(subkey, (batch_size, input_size), dtype=jnp.float32)\n",
    "# input = jax.device_put(input)\n",
    "\n",
    "# # Create mmore realistic indx seqs by randomly shuffling and sampling\n",
    "# indx_seqs_stack = []\n",
    "# for i in range(layer_width):\n",
    "#   key, subkey = random.split(key)\n",
    "#   key, subkey2 = random.split(key)\n",
    "#   indx_seqs_stack.append(jax.random.shuffle(subkey, jax.random.choice(subkey2, jnp.arange(input_size), (sparsity,))))\n",
    "# indx_seqs = jnp.stack(indx_seqs_stack)\n",
    "# indx_seqs = jax.device_put(indx_seqs)\n",
    "\n",
    "# key, subkey = random.split(key)\n",
    "# weights = random.uniform(subkey, (layer_width, sparsity))\n",
    "\n",
    "weights, bias, indx_seqs = condensed_params['params']['kernel'], condensed_params['params']['bias'], condensed_params['params']['indx_seqs']\n",
    "weights = jax.device_put(weights)\n",
    "bias = jax.device_put(bias)\n",
    "indx_seqs = jax.device_put(indx_seqs)\n",
    "input = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 768)\n",
      "(1145, 206)\n",
      "(1145, 206)\n"
     ]
    }
   ],
   "source": [
    "for a in [input, weights, indx_seqs]:\n",
    "  print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.75584006, -0.17063873,  1.6859026 , ...,  0.18504576,\n",
       "        -0.00192568,  0.08196551],\n",
       "       [-0.02017869, -0.71082234,  0.16039723, ...,  0.41155237,\n",
       "        -0.4476214 ,  1.0754542 ],\n",
       "       [ 0.7968303 , -0.10603629,  1.2908362 , ..., -0.32634115,\n",
       "        -0.48441118, -0.49104658],\n",
       "       ...,\n",
       "       [ 0.30003327, -0.17096956,  0.10126591, ...,  0.6488617 ,\n",
       "        -0.1293261 , -0.16757864],\n",
       "       [-0.01557497, -0.39440617,  0.21288827, ...,  0.32918108,\n",
       "        -0.12092257, -0.527568  ],\n",
       "       [-0.22859119, -0.54928684,  0.2176865 , ...,  0.11042877,\n",
       "         0.5686784 ,  0.13307133]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward_orig(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.sum(weights * input[:, indx_seqs], axis=2) + bias\n",
    "\n",
    "forward_orig_fast = jax.jit(forward_orig)\n",
    "forward_orig_fast(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.75584006, -0.17063873,  1.6859026 , ...,  0.18504576,\n",
       "        -0.00192568,  0.08196551],\n",
       "       [-0.02017869, -0.71082234,  0.16039723, ...,  0.41155237,\n",
       "        -0.4476214 ,  1.0754542 ],\n",
       "       [ 0.7968303 , -0.10603629,  1.2908362 , ..., -0.32634115,\n",
       "        -0.48441118, -0.49104658],\n",
       "       ...,\n",
       "       [ 0.30003327, -0.17096956,  0.10126591, ...,  0.6488617 ,\n",
       "        -0.1293261 , -0.16757864],\n",
       "       [-0.01557497, -0.39440617,  0.21288827, ...,  0.32918108,\n",
       "        -0.12092257, -0.527568  ],\n",
       "       [-0.22859119, -0.54928684,  0.2176865 , ...,  0.11042877,\n",
       "         0.5686784 ,  0.13307133]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_orig_fast(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.69 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward_orig(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73 ms ± 32.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward_orig_fast(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_orig_faster = jax.jit(partial(forward_orig, weights=weights, indx_seqs=indx_seqs, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.75584006, -0.17063873,  1.6859026 , ...,  0.18504576,\n",
       "        -0.00192568,  0.08196551],\n",
       "       [-0.02017869, -0.71082234,  0.16039723, ...,  0.41155237,\n",
       "        -0.4476214 ,  1.0754542 ],\n",
       "       [ 0.7968303 , -0.10603629,  1.2908362 , ..., -0.32634115,\n",
       "        -0.48441118, -0.49104658],\n",
       "       ...,\n",
       "       [ 0.30003327, -0.17096956,  0.10126591, ...,  0.6488617 ,\n",
       "        -0.1293261 , -0.16757864],\n",
       "       [-0.01557497, -0.39440617,  0.21288827, ...,  0.32918108,\n",
       "        -0.12092257, -0.527568  ],\n",
       "       [-0.22859119, -0.54928684,  0.2176865 , ...,  0.11042877,\n",
       "         0.5686784 ,  0.13307133]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_orig_faster(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.63 ms ± 30.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit forward_orig_faster(input).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*italicized text*## Method #1: Use slicing/indexing and broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_output = forward_orig(input, weights, indx_seqs, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do forward pass for a single neuron from a single batch\n",
    "def forward_neuron_single(input: jnp.ndarray, weights: jnp.ndarray, indices: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.sum(input[indices] * weights)\n",
    "\n",
    "def forward_neuron_v(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
    "    return vmap(partial(forward_neuron_single, input), in_axes=0, out_axes=0)(weights, indx_seqs) + bias\n",
    "\n",
    "def forward_neuron(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
    "    return vmap(partial(forward_neuron_v, weights=weights, indx_seqs=indx_seqs, bias=bias))(input)\n",
    "\n",
    "# Do forward pass for all neurons over sparsity axis from a single batch\n",
    "def forward_sparsity_single(input: jnp.ndarray, weights: jnp.ndarray, indices: jnp.ndarray) -> jnp.ndarray:\n",
    "    return input[indices] * weights\n",
    "\n",
    "def forward_sparsity_v(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
    "    output_neurons = vmap(partial(forward_sparsity_single, input), in_axes=1, out_axes=1)(weights, indx_seqs)\n",
    "    return jnp.sum(output_neurons, axis=1) + bias\n",
    "\n",
    "def forward_sparsity(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
    "    return vmap(partial(forward_sparsity_v, weights=weights, indx_seqs=indx_seqs, bias=bias))(input)\n",
    "\n",
    "forward_neuron_fast = jax.jit(forward_neuron)\n",
    "forward_neuron_faster = jax.jit(partial(forward_neuron, weights=weights, indx_seqs=indx_seqs, bias=bias))\n",
    "forward_sparsity_fast = jax.jit(forward_sparsity)\n",
    "forward_sparsity_faster = jax.jit(partial(forward_sparsity, weights=weights, indx_seqs=indx_seqs, bias=bias))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #2: vmap over neuron/sparsity axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/mike/condensed-sparsity/notebooks/jax_benchmarks.ipynb Cell 26\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhector/home/mike/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m fast_sparsity_output \u001b[39m=\u001b[39m forward_sparsity_fast(\u001b[39minput\u001b[39m, weights, indx_seqs, bias)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhector/home/mike/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m fast_sparsity_output_faster \u001b[39m=\u001b[39m forward_sparsity_faster(\u001b[39minput\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhector/home/mike/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m jnp\u001b[39m.\u001b[39mallclose(orig_output, fast_sparsity_output)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhector/home/mike/condensed-sparsity/notebooks/jax_benchmarks.ipynb#Y100sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m jnp\u001b[39m.\u001b[39mallclose(orig_output, fast_sparsity_output_faster)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# call once so JIT happens\n",
    "fast_sparsity_output = forward_sparsity_fast(input, weights, indx_seqs, bias)\n",
    "fast_sparsity_output_faster = forward_sparsity_faster(input)\n",
    "assert jnp.allclose(orig_output, fast_sparsity_output)\n",
    "assert jnp.allclose(orig_output, fast_sparsity_output_faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_sparsity(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_sparsity_fast(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_sparsity_faster(input).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method #3: vmap over sparsity/neuron axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call once so JIT happens\n",
    "fast_neuron_output = forward_neuron_fast(input, weights, indx_seqs, bias)\n",
    "faster_neuron_output = forward_neuron_faster(input)\n",
    "assert jnp.allclose(orig_output, fast_neuron_output)\n",
    "assert jnp.allclose(orig_output, faster_neuron_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_neuron(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_neuron_fast(input, weights, indx_seqs, bias).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_neuron_faster(input).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondensedLinearVmapNeuron(nn.Module):\n",
    "    features: int\n",
    "    fan_in: int\n",
    "    kernel_init: Callable = nn.initializers.lecun_normal()\n",
    "    bias_init: Callable = nn.initializers.zeros_init()\n",
    "\n",
    "    def forward_neuron(self, input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray, bias: jnp.ndarray) -> jnp.ndarray:\n",
    "      return vmap(partial(forward_neuron_v, weights=weights, indx_seqs=indx_seqs, bias=bias))(input)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input: ArrayLike) -> jax.Array:\n",
    "        kernel = self.param(\"kernel\", self.kernel_init, (self.features, self.fan_in))\n",
    "        bias = self.param(\"bias\", self.bias_init, (self.features,))\n",
    "        indx_seqs = self.param(\"indx_seqs\", self.kernel_init, (self.features, self.fan_in))\n",
    "        return self.forward_neuron(input, weights, indx_seqs, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = CondensedLinearVmapNeuron(*condensed_params[\"params\"][\"kernel\"].shape)\n",
    "cl_fast = jax.jit(lambda x: cl.apply(condensed_params, x))\n",
    "cl_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cl_fast(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit cl.apply(condensed_params, x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do forward pass for a single neuron from a single batch\n",
    "def forward_batch_neuron_single(input: jnp.ndarray, weights: jnp.ndarray, indices: jnp.ndarray) -> jnp.ndarray:\n",
    "    return jnp.sum(input[:, indices] * weights[None, :], axis=1)\n",
    "\n",
    "def forward_batch_neuron(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray) -> jnp.ndarray:\n",
    "    return vmap(partial(forward_batch_neuron_single, input), in_axes=0, out_axes=0)(weights, indx_seqs).T\n",
    "\n",
    "def forward_batch_sparsity(input: jnp.ndarray, weights: jnp.ndarray, indx_seqs: jnp.ndarray) -> jnp.ndarray:\n",
    "    return vmap(partial(forward_batch_neuron_single, input), in_axes=0, out_axes=1)(weights, indx_seqs)\n",
    "\n",
    "forward_batch_neuron_fast = jax.jit(forward_batch_neuron)\n",
    "forward_batch_sparsity_fast = jax.jit(forward_batch_sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_batch_neuron_output = forward_batch_neuron_fast(input, weights, indx_seqs)\n",
    "forward_batch_sparsity_fast_output = forward_batch_sparsity_fast(input, weights, indx_seqs)\n",
    "assert jnp.allclose(orig_output, forward_batch_neuron_output)  ## TODO: Add bias to above\n",
    "assert jnp.allclose(orig_output, forward_batch_sparsity_fast_output)  ## TODO: Add bias to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_batch_neuron(input, weights, indx_seqs).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit forward_batch_neuron_fast(input, weights, indx_seqs).block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
