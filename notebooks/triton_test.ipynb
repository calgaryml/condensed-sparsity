{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MaskedWeight():\n",
    "    mask: torch.Tensor\n",
    "    weight: torch.Tensor\n",
    "\n",
    "def get_const_fan_w_and_M(num_neurons: int = 10, dense_fan_in: int = 100, sparsity: float = 0.1) -> MaskedWeight:\n",
    "    m = _generate_mask(num_neurons, dense_fan_in, sparsity)\n",
    "    w = torch.rand(size=(num_neurons, dense_fan_in), dtype=torch.float32)\n",
    "    w = w * m\n",
    "    return MaskedWeight(m, w)\n",
    "\n",
    "def _generate_mask(num_neurons, dense_fan_in, sparsity):\n",
    "    m = torch.zeros(size=(num_neurons, dense_fan_in), dtype=torch.bool)\n",
    "    num_ones = (m.numel() * sparsity).__floor__()\n",
    "    sparse_fan_in = num_ones // num_neurons\n",
    "    for neuron_idx, neuron in enumerate(m):\n",
    "        ones_idx = torch.randperm(len(neuron))\n",
    "        m[neuron_idx][ones_idx[:sparse_fan_in]] = True\n",
    "    return m\n",
    "\n",
    "def get_input(num_features: int = 10, num_batches: int = 1) -> torch.Tensor:\n",
    "    return torch.rand(size=(num_batches, num_features))\n",
    "    \n",
    "    \n",
    "mw = get_const_fan_w_and_M(3,10,0.33)\n",
    "input = get_input(10,1)\n",
    "(input@mw.weight.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1769,  0.0034, -0.1144]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input @ nn.Linear(in_features=10, out_features=3).weight.data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "tensor([1.3713, 1.3076, 0.4940, 1.2701, 1.2803, 1.1750, 1.1790, 1.4607, 0.3393,\n",
      "        1.2689], device='cuda:0')\n",
      "tensor([1.3713, 1.3076, 0.4940, 1.2701, 1.2803, 1.1750, 1.1790, 1.4607, 0.3393,\n",
      "        1.2689], device='cuda:0')\n",
      "The maximum difference between torch and triton is 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,  # *Pointer* to first input vector.\n",
    "    y_ptr,  # *Pointer* to second input vector.\n",
    "    output_ptr,  # *Pointer* to output vector.\n",
    "    n_elements,  # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "                 # NOTE: `constexpr` so it can be used as a shape value.\n",
    "):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    if pid ==0:\n",
    "        pass\n",
    "        # tl.device_print(\"pid:\", pid)\n",
    "    # tl.device_print(f\"pid\", pid)\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "    # tl.device_print(\"output ptr + offsets: \", output_ptr+offsets)\n",
    "    # if pid==0:\n",
    "    #     # pass\n",
    "    #     tl.device_print(\"block_start\", block_start)\n",
    "    #     tl.device_print(\"offsets\", offsets)\n",
    "    #     tl.device_print(\"mask\", mask)\n",
    "    \n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n",
    "    print(grid(meta={\"BLOCK_SIZE\":1024}))\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "size = 10\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(\n",
    "    f'The maximum difference between torch and triton is '\n",
    "    f'{torch.max(torch.abs(output_torch - output_triton))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "pid: 0\n",
      "tensor([[0.9988, 0.8174, 0.1544, 0.6956, 0.8776, 0.9998, 0.9372, 0.8874, 0.3854,\n",
      "         0.3245, 0.9105, 0.7802, 0.1991, 0.9495, 0.7416, 0.7726]],\n",
      "       device='cuda:0')\n",
      "tensor([[0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "## Row major ordering\n",
    "def main(\n",
    "    num_features = 16, num_neurons = 2, sparsity = 0.5, num_batches=1\n",
    "):\n",
    "    mw = get_const_fan_w_and_M(num_neurons, num_features, sparsity)\n",
    "    x = get_input(num_features, num_batches)\n",
    "    mw.weight = mw.weight.to(\"cuda:0\")\n",
    "    x = x.to(\"cuda:0\")\n",
    "    mw.mask = mw.mask.to(\"cuda:0\")\n",
    "    try:\n",
    "        x@mw.weight.T\n",
    "    except Exception as e:\n",
    "        raise e \n",
    "    _META = dict(BLOCK_SIZE=mw.weight.shape[1])\n",
    "    nm_matmul(_META, W_T=mw.weight.T, x=x, mask_T=mw.mask.T)\n",
    "    \n",
    "\n",
    "def nm_matmul(_META: dict[str, any], W_T: torch.Tensor, x: torch.Tensor, mask_T :torch.Tensor) -> torch.Tensor:\n",
    "    assert W_T.is_cuda and x.is_cuda and mask_T.is_cuda\n",
    "    M,K = x.shape\n",
    "    K,N = W_T.shape\n",
    "    y: torch.Tensor = torch.zeros((M,N), dtype=x.dtype, device=x.device)\n",
    "    debug_x = torch.zeros_like(x, device=x.device)\n",
    "    grid = (M,) # 1D grid\n",
    "    grid = lambda meta: (M,)\n",
    "    grid = (1,)\n",
    "    BLOCK_SIZE_M = triton.next_power_of_2(M)\n",
    "    BLOCK_SIZE_N = triton.next_power_of_2(N)\n",
    "    BLOCK_SIZE_K = triton.next_power_of_2(K)\n",
    "    _ = _nm_matmul[grid](  # We index each kernel by number of neurons in W \n",
    "        W_T, x, y, mask_T, debug_x,  # pointers to tensors\n",
    "        M, N, K,  # Shape of tensors ( (MxK * KxN = MxN) )\n",
    "        stride_wk=W_T.stride(0), stride_wn=W_T.stride(1),\n",
    "        stride_xm=x.stride(0), stride_xk=x.stride(1),\n",
    "        stride_ym=y.stride(0), stride_yn=y.stride(1),\n",
    "        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N,\n",
    "        BLOCK_SIZE_K=BLOCK_SIZE_K,\n",
    "        # GROUP_SIZE_M=None,\n",
    "        num_warps=2,  # 32 threads per warp\n",
    "    )\n",
    "    # print(mask_T)\n",
    "    # print(debug_m)  \n",
    "    print(x)\n",
    "    print(y)  \n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _nm_matmul(\n",
    "    w_ptr,  # Shape (in_features * num_neurons)  (K x N)\n",
    "    x_ptr,  # Shape (num_batches, in_features)  (M x K)\n",
    "    y_ptr,  # Shape (num_batches * num_neurons)  (M x N)\n",
    "    mask_ptr,  # Shape (in_features * num_neurons)  (M x N)\n",
    "    debug_ptr,\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    stride_wk, stride_wn,\n",
    "    stride_xm, stride_xk,\n",
    "    stride_ym, stride_yn,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    # GROUP_SIZE_M,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    tl.device_print(\"pid: \", pid)\n",
    "    offset_xm = p * BLOCK_SIZE_M + tl.arange(0,BLOCK_SIZE_M)\n",
    "    offset_xm = tl.arange(0, 16)\n",
    "    # tl.device_print(\"offset_xm: \", offset_xm)\n",
    "    # row_start_index = pid * K\n",
    "    # offsets = row_start_index * tl.arange(0,BLOCK_SIZE_K)\n",
    "    # x_ptrs = x_ptr + offsets\n",
    "    # debug_ptrs = debug_ptr + offsets\n",
    "    # load_mask = x_ptrs > (row_start_index+K)\n",
    "    # x = tl.load(x_ptrs, mask=load_mask)\n",
    "    # tl.store(debug_ptrs, x, mask=load_mask)\n",
    "    return\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # # Multiplication in form X@W.T\n",
    "    # pid = tl.program_id(axis=0)  # 1D launch grid (index of row of Y)\n",
    "    # mask_start_idx = pid * K  # Move pointer to start     tl.device_print(\"PID: \", pid)wk \n",
    "    # debug_ptrs = debug_ptr + mask_offsets[:,None]*stride_wk  \n",
    "    # # tl.device_print(\"mask ptrs: \", mask_ptrs)\n",
    "    # # tl.device_print(\"debug ptrs: \", debug_ptrs)\n",
    "    # load_mask = mask_offsets < N\n",
    "    # # mask = tl.load(mask_ptrs, mask=load_mask)\n",
    "    # # tl.device_print(\"mask:\", mask+mask_offsets)\n",
    "    # # tl.store(debug_ptrs, mask, mask=load_mask)\n",
    "    \n",
    "    \n",
    "    # # tl.device_print(f\"pid\", pid)\n",
    "    # # This program will process inputs that are offset from the initial data.\n",
    "    # # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # # Note that offsets is a list of pointers:\n",
    "    # block_start = pid * BLOCK_SIZE\n",
    "    # offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # # Create a mask to guard memory operations against otut-of-bourelevant\n",
    "    # x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # tl.static_print(mask)\n",
    "    # non_zero_w = tl.load(w_ptr, mask=mask[row_idx])\n",
    "    # tl.device_print(non_zero_w)\n",
    "    return\n",
    "    # row_start_ptr = w_ptr+row_idx*w_n_cols,\n",
    "\n",
    "    # block_start = pid * \n",
    "    # mask = tl.load(m_ptr)\n",
    "    # w_sp = tl.load(w_ptr, mask=m_ptr)\n",
    "    # tl.store(w_ptr @ x_ptr)\n",
    "    # print(type(w_ptr))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3771184639.py, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 65\u001b[0;36m\u001b[0m\n\u001b[0;31m    block_start = pid *\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Group Ordering\n",
    "\n",
    "def main(\n",
    "    num_features = 10, num_neurons = 10, sparsity = 0.1, num_batches=1\n",
    "):\n",
    "    mw = get_const_fan_w_and_M(num_neurons, num_features, sparsity)\n",
    "    x = get_input(num_features, num_batches)\n",
    "    mw.weight = mw.weight.to(\"cuda:0\")\n",
    "    x = x.to(\"cuda:0\")\n",
    "    mw.mask = mw.mask.to(\"cuda:0\")\n",
    "    try:\n",
    "        x@mw.weight.T\n",
    "    except Exception as e:\n",
    "        raise e \n",
    "    _META = dict(BLOCK_SIZE=mw.weight.shape[1])\n",
    "    nm_matmul(_META, W=mw.weight, x=x, m=m)\n",
    "    \n",
    "\n",
    "def nm_matmul(_META: dict[str, any], W: torch.Tensor, x: torch.Tensor, m:torch.Tensor) -> torch.Tensor:\n",
    "    assert W.is_cuda and x.is_cuda and m.is_cuda\n",
    "    # num_elements = y.numel()\n",
    "    # grid = lambda meta: (triton.cdiv(num_elements, _META[\"BLOCK_SIZE\"]))  # ceiling division yields number of \"programs\" to be launched\n",
    "    # BLOCK_SIZE = triton.next_power_of_2(W.shape[1])\n",
    "    # num_warps = 4\n",
    "    M,K = x.shape\n",
    "    K,N = W.T.shape\n",
    "    y: torch.Tensor = torch.empty_like((M,N), dtype=x.dtype, device=x.device)\n",
    "    BLOCK_SIZE_M = 8\n",
    "    BLOCK_SIZE_N = 8\n",
    "    BLOCK_SIZE_K = 8\n",
    "    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]), triton.cdiv(N, META[\"BLOCK_SIZE_N\"]))  # 2D grid\n",
    "    _nm_matmul[grid](  # We index each kernel by number of neurons in W \n",
    "        W.T, x, y,  # pointers to tensors\n",
    "        M, N, K,  # Shape of tensors ( (MxK * KxN = MxN) )\n",
    "        stride_wk=W.T.stride(0), stride_wn=W.T.stride(1),\n",
    "        stride_xm=x.stride(0), stride_xk=x.stride(1),\n",
    "        stride_ym=y.stride(0), stride_yn=y.stride(1),\n",
    "        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_K,\n",
    "        BLOCK_SIZE_K=BLOCK_SIZE_K, GROUP_SIZE_M=None,\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _nm_matmul(\n",
    "    w_ptr,  # Shape (in_features * num_neurons)\n",
    "    x_ptr,  # Shape (num_batches, in_features)\n",
    "    y_ptr,  # Shape (num_batches * num_neurons)\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    stride_wk, stride_wn,\n",
    "    stride_xm, stride_xk,\n",
    "    stride_ym, stride_yn,\n",
    "    BLOCK_SIZE_M,\n",
    "    BLOCK_SIZE_N,\n",
    "    BLOCK_SIZE_K,\n",
    "    GROUP_SIZE_M,\n",
    "):\n",
    "    # Multiplication in form X@W.T\n",
    "    row_idx = tl.program_id(axis=0)  # 1D launch grid\n",
    "    row_start_ptr = w_ptr+row_idx*w_n_cols,\n",
    "\n",
    "    block_start = pid * \n",
    "    mask = tl.load(m_ptr)\n",
    "    w_sp = tl.load(w_ptr, mask=m_ptr)\n",
    "    tl.store(w_ptr @ x_ptr)\n",
    "    print(type(w_ptr))\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
