# compute config group

# If True, use data-parallelization model for distributed training
distributed: True
# Number of CUDA devices to use for distributed training
world_size: 4
